@inproceedings{aboaf1989task,
 author = {Aboaf, Eric W and Drucker, Steven M and Atkeson, Christopher G},
 booktitle = {Robotics and Automation, 1989. Proceedings., 1989 IEEE International Conference on},
 caption = {Task Level Learning},
 editor = {},
 facet.collaborators = {Aboaf,Atkeson},
 facet.publication = {IEEE,MastersThesis},
 facet.subject = {Robotics,Learning,Thesis},
 facet.year = {1989},
 id = {4.0},
 img = {researchImages/robotlearn.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {IEEE},
 abstract = {We report on a preliminary investigation of task-level learning, an approach to learning from practice. We have a programmaed a robot to juggle a single ball in three dimensions by batting it upwards with a large paddle. The robot uses a real-time binary vision system to track the ball and measure its performance. Task-level learning consists of building a model of performance errors at the task level during practice, and using that model to refine task-level commands. A polynomial surface was fit to the errors in where the ball went after each hit, and this task model is used to refine how the ball is hit. This application of task-level learning dramatically increased the number of consecutive hits the robot could execute before the ball was hit out of range of the paddle.},
 pages = {1290--1295},
 pdf = {http://research.microsoft.com/copyright/accept.asp?path=/~sdrucker/papers/tasklevel.pdf&pub=IEEE},
 primary = {Robotics},
 publisher = {},
 reference = {Aboaf, E, Drucker, S.M, and Atkeson, C.G. Task Level Learning on a Juggling Task. IEEE Robotics Conference. Scottsdale, AZ. 1989.},
 school = {},
 text = {tasklevel.txt},
 thumb = {thumbnail/robotlearn.gif},
 title = {Task-level robot learning: Juggling a tennis ball more accurately},
 type = {},
 video = {},
 volume = {},
 year = {1989}
}

@inproceedings{agarwala2004interactive,
 author = {Agarwala, Aseem and Dontcheva, Mira and Agrawala, Maneesh and Drucker, Steven and Colburn, Alex and Curless, Brian and Salesin, David and Cohen, Michael},
 booktitle = {ACM Transactions on Graphics (TOG)},
 caption = {Interactive Digital Photomontage},
 editor = {},
 facet.collaborators = {Agarwala,Dontcheva,Agrawala,Colburn,Curless,Salesin,Cohen},
 facet.publication = {SIGGRAPH},
 facet.subject = {Graphics,Photos},
 facet.year = {2004},
 id = {33.0},
 img = {researchImages/photomontage.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {3},
 organization = {ACM},
 abstract = {We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call 'digital photomontage.' Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as 'maximum contrast') drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including 'selective composites' (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics.},
 pages = {294--302},
 pdf = {http://research.microsoft.com/~sdrucker/papers/photomontage.pdf},
 primary = {Photos},
 publisher = {},
 reference = {Aseem Agarwala, Mira Dontcheva, Maneesh Agrawala, Steven Drucker, Alex Colburn, Brian Curless, David Salesin, Michael Cohen. Interactive Digital Photomontage. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2004), 2004.},
 school = {},
 text = {photomontage.txt},
 thumb = {thumbnail/photomontage.jpg},
 title = {Interactive digital photomontage},
 type = {},
 video = {http://grail.cs.washington.edu/projects/photomontage/video.avi},
 volume = {23},
 year = {2004}
}

@inproceedings{ali2011online,
 author = {Ali, Mohamed and Chandramouli, Badrish and Fay, Jonathan and Wong, Curtis and Drucker, Steven and Raman, Balan Sethu},
 booktitle = {Proceedings of the International Conference on Very Large Data Bases (VLDB)},
 caption = {Geospatial Stream},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {},
 pages = {},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Online visualization of geospatial stream data using the WorldWide telescope},
 type = {},
 video = {},
 volume = {},
 year = {2011}
}

@inproceedings{amershi2015modeltracker,
 author = {Amershi, Saleema and Chickering, Max and Drucker, Steven M and Lee, Bongshin and Simard, Patrice and Suh, Jina},
 booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
 caption = {Modeltracker},
 editor = {},
 facet.collaborators = {Amershi,Chickering,Lee,Simard,Suh},
 facet.publication = {SIGCHI},
 facet.subject = {Information,Visualization,Touch,Sequences},
 facet.year = {2015},
 id = {78.0},
 img = {researchImages/modeltracker.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present ModelTracker, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with  ModelTracker over six months shows ModelTracker is used often and throughout model building. A controlled experiment focusing on ModelTracker???s debugging capabilities shows participants prefer ModelTracker over traditional tools without a loss in model performance.},
 pages = {337--346},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/pn2048-amershi-fixed.pdf},
 primary = {Machine Learning},
 publisher = {},
 reference = {Amershi, S., Chickering, M., Drucker, S., Lee, B., Simard, P., and Suh, J. (2015) ModelTracker: Redesigning Performance Analysis Tools for Machine Learning. Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI 2015).},
 school = {},
 text = {pn2048-amershi-fixed.txt},
 thumb = {thumbnail/modeltracker.png},
 title = {Modeltracker: Redesigning performance analysis tools for machine learning},
 type = {},
 video = {http://research.microsoft.com/en-us/um/people/sdrucker/video/squeries_v1.0.mp4},
 volume = {},
 year = {2015}
}

@unpublished{andre2007informal,
 author = {Paul Andr{\'e} and Steven Drucker and m.c. schraefel},
 booktitle = {CHI2008},
 caption = {Informal Decisions},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {Microsoft, University of Southampton},
 journal = {},
 link = {http://eprints.soton.ac.uk/264545/},
 month = {},
 number = {},
 organization = {},
 abstract = {Existing group decision support systems are too complex to support lightweight, informal decision making made popular by the amount of information available on the Web. From an examination of related work, an online survey and a formative study to examine how people currently use the Web for decision support, we present a set of design recommendations towards the development of an informal Web decision support tool.},
 pages = {},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Informal Online Decision Making: Current Practices and Support System Design},
 type = {Technical Report},
 video = {},
 volume = {},
 year = {2007}
}

@inproceedings{barik2016bones,
 author = {Barik, Titus and DeLine, Robert and Drucker, Steven and Fisher, Danyel},
 booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
 caption = {Bones},
 editor = {},
 facet.collaborators = {Barik,DeLine,Fisher},
 facet.publication = {ICSE},
 facet.subject = {Information},
 facet.year = {2016},
 id = {83.0},
 img = {researchImages/bones.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Large software organizations are transitioning to event data platforms as they culturally shift to better support datadriven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in datadriven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.},
 pages = {92--101},
 pdf = {http://static.barik.net/barik/publications/icse2016/PID4092213.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Barik, T., DeLine, R., Drucker, S., & Fisher, D. (2016, May). The bones of the system: a case study of logging and telemetry at Microsoft. In?Proceedings of the 38th International Conference on Software Engineering Companion?(pp. 92-101). ACM.},
 school = {},
 text = {bonesofthesystem.txt},
 thumb = {thumbnail/bones.png},
 title = {The bones of the system: a case study of logging and telemetry at Microsoft},
 type = {},
 video = {},
 volume = {},
 year = {2016}
}

@inproceedings{barnett2013stat,
 author = {Barnett, Mike and Chandramouli, Badrish and DeLine, Robert and Drucker, Steven and Fisher, Danyel and Goldstein, Jonathan and Morrison, Patrick and Platt, John},
 booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
 caption = {Stat!},
 editor = {},
 facet.collaborators = {Barnett,Chandramouli,DeLine,Fisher,Goldstein,Morrison,Platt},
 facet.publication = {Sigmod},
 facet.subject = {Information},
 facet.year = {2013},
 id = {70.0},
 img = {researchImages/stat.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Exploratory analysis on big data requires us to rethink data management across the entire stack ??? from the underlying data processing techniques to the user experience. We demonstrate Stat! ??? a visualization and analytics environment that allows users to rapidly experiment with exploratory queries over big data. Data scientists can use Stat! to quickly refine to the correct query, while getting immediate feedback after processing a fraction of the data. Stat! can work with multiple processing engines in the backend; in this demo, we use Stat! with the Microsoft StreamInsight streaming engine. StreamInsight is used to generate incremental early results to queries and refine these results as more data is processed. Stat! allows data scientists to explore data, dynamically compose multiple queries to generate streams of partial results, and display partial results in both textual and visual form},
 pages = {1013--1016},
 pdf = {http://research.microsoft.com/pubs/194060/stat-sigmod2013-demo.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Mike Barnett, Badrish Chandramouli, Robert DeLine, Steven Drucker, Danyel Fisher, Jonathan Goldstein, Patrick Morrison, and John Platt, Stat! - An Interactive Analytics Environment for Big Data, in ACM SIGMOD International Conference on Management of Data (SIGMOD 2013), ACM SIGMOD, June 2013},
 school = {},
 text = {sigde260-barnett.txt},
 thumb = {thumbnail/stat.png},
 title = {Stat!: an interactive analytics environment for big data},
 type = {},
 video = {http://research.microsoft.com/en-us/people/sdrucker/papers/},
 volume = {},
 year = {2013}
}

@inproceedings{basu2010assisting,
 author = {Basu, Sumit and Fisher, Danyel and Drucker, Steven M and Lu, Hao},
 booktitle = {AAAI},
 caption = {iClusterTheory},
 editor = {},
 facet.collaborators = {Fisher,Basu,Lu},
 facet.publication = {AAAI},
 facet.subject = {UI,Machine Learning},
 facet.year = {2010},
 id = {56.0},
 img = {researchImages/iclustering2.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Interactive clustering refers to situations in which a human labeler is willing to assist a learning algorithm in automatically clustering items. We present a related but somewhat different task, assisted clustering, in which a user creates explicit groups of items from a large set and wants suggestions on what items to add to each group. While the traditional approach to interactive clustering has been to use metric learning to induce a distance metric, our situation seems equally amenable to classification. Using clusterings of documents from human subjects, we found that one or the other method proved to be superior for a given cluster, but not uniformly so. We thus developed a hybrid mechanism for combining the metric learner and the classifier. We present results from a large number of trials based on human clusterings, in which we show that our combination scheme matches and often exceeds the performance of a method which exclusively uses either type of learner.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/iclustering-aaai-2010.pdf},
 primary = {Machine Learning},
 publisher = {},
 reference = {Sumit Basu, Danyel Fisher, Steven M. Drucker, and Hao Lu, Assisting Users with Clustering Tasks by Combining Metric Learning and Classification, in Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010), American Association for Artificial Intelligence , July 2010},
 school = {},
 text = {iClustering-aaai-2010.txt},
 thumb = {thumbnail/iclustering2.png},
 title = {Assisting Users with Clustering Tasks by Combining Metric Learning and Classification.},
 type = {},
 video = {},
 volume = {},
 year = {2010}
}

@inproceedings{bigelow2014reflections,
 author = {Bigelow, Alex and Drucker, Steven and Fisher, Danyel and Meyer, Miriah},
 booktitle = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
 caption = {DesignReflections},
 editor = {},
 facet.collaborators = {Bigelow,Fisher,Meyer},
 facet.publication = {AVI},
 facet.subject = {Visualization,Information,Design},
 facet.year = {2013},
 id = {67.0},
 img = {researchImages/DesignReflections.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {In recent years many popular data visualizations have emerged that are created largely by designers whose main area of expertise is not computer science. Designers generate these visualizations using a handful of design tools and environments. To better inform the development of tools intended for designers working with data, we set out to understand designers' challenges and perspectives. We interviewed professional designers, conducted observations of designers working with data in the lab, and observed designers working with data in team settings in the wild. A set of patterns emerged from these observations from which we extract a number of themes that provide a new perspective on design considerations for visualization tool creators, as well as on known engineering problems.},
 pages = {17--24},
 pdf = {http://research.microsoft.com/apps/pubs/default.aspx?id=217732},
 primary = {Visualization},
 publisher = {},
 reference = {Alex Bigelow, Steven Drucker, Danyel Fisher, and Miriah Meyer, Reflections on How Designers Design With Data, in AVI 2014 International Working Conference on Advanced Visual Interfaces, ACM, May 2014},
 school = {},
 text = {2014-avi-design-vis.txt},
 thumb = {thumbnail/DesignReflections.png},
 title = {Reflections on how designers design with data},
 type = {},
 video = {http://research.microsoft.com/apps/pubs/default.aspx?id=208568},
 volume = {},
 year = {2014}
}

@article{bigelow2017iterating,
 author = {Bigelow, Alex and Drucker, Steven and Fisher, Danyel and Meyer, Miriah},
 booktitle = {},
 caption = {Hanpuku},
 editor = {},
 facet.collaborators = {Bigelow,Fisher,Meyer},
 facet.publication = {Infovis},
 facet.subject = {Information,Visualization,Design},
 facet.year = {2016},
 id = {81.0},
 img = {researchImages/hanpuku.png},
 institution = {},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 link = {},
 month = {},
 number = {1},
 organization = {},
 abstract = {A common work?ow for visualization designers begins with a generative tool, like D3 or Processing, to create the initial visualization;andproceedstoadrawingtool,likeAdobeIllustratororInkscape,foreditingandcleaning. Unfortunately,thisistypically a one-way process: once a visualization is exported from the generative tool into a drawing tool, it is dif?cult to make further, datadriven changes. In this paper, we propose a bridge model to allow designers to bring their work back from the drawing tool to re-edit in the generative tool. Our key insight is to recast this iteration challenge as a merge problem - similar to when two people are editing a document and changes between them need to reconciled. We also present a speci?c instantiation of this model, a tool called Hanpuku, which bridges between D3 scripts and Illustrator. We show several examples of visualizations that are iteratively created using Hanpuku in order to illustrate the ?exibility of the approach. We further describe several hypothetical tools that bridge between other visualization tools to emphasize the generality of the model. },
 pages = {481--490},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/hanpuku.pdf},
 primary = {Visualization},
 publisher = {IEEE},
 reference = {Bigelow, A., Drucker, S., Fisher, D., & Meyer, M. (2017). Iterating between tools to create and edit visualizations.?IEEE Transactions on Visualization and Computer Graphics,?23(1), 481-490.},
 school = {},
 text = {hanpuku.txt},
 thumb = {thumbnail/hanpuku.png},
 title = {Iterating between tools to create and edit visualizations},
 type = {},
 video = {},
 volume = {23},
 year = {2017}
}

@inproceedings{brooks2015featureinsight,
 author = {Brooks, Michael and Amershi, Saleema and Lee, Bongshin and Drucker, Steven M and Kapoor, Ashish and Simard, Patrice},
 booktitle = {Visual Analytics Science and Technology (VAST), 2015 IEEE Conference on},
 caption = {},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {IEEE},
 caption={Feature Insight},
 abstract = {},
 pages = {105--112},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {FeatureInsight: Visual support for error-driven feature ideation in text classification},
 type = {},
 video = {},
 volume = {},
 year = {2015}
}

@techreport{chen2008interactive,
 author = {Chen, Billy and Ramos, Gonzalo and Ofek, Eyal and Cohen, Michael and Drucker, Steven and Nist{\'e}r, David},
 booktitle = {},
 caption = {},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {Microsoft Research},
 caption={Terrain Registration},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {},
 pages = {},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Interactive techniques for registering images to digital terrain and building models},
 type = {},
 video = {},
 volume = {},
 year = {2008}
}

@inproceedings{chi2014demowiz,
 author = {Chi, Pei-Yu and Lee, Bongshin and Drucker, Steven M},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 caption = {Demowiz},
 editor = {},
 facet.collaborators = {Chi,Lee},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Presentation},
 facet.year = {2014},
 id = {76.0},
 img = {researchImages/demowiz.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Showing a live software demonstration during a talk can be engaging, but it is often not easy: presenters may struggle with (or worry about) unexpected software crashes and  encounter issues such as mismatched screen resolutions or faulty network connectivity.  Furthermore, it can be difficult to recall the steps to show while talking and operating the  system all at the same time. An alternative is to present with pre-recorded screencast videos. It is, however, challenging to precisely match the narration to the video when using existing video players. We introduce DemoWiz, a video presentation system that provides an increased awareness of upcoming actions through glanceable visualizations. DemoWiz supports better control of timing by overlaying visual cues and enabling lightweight editing. A user study shows that our design significantly improves the presenters??? perceived ease of narration and timing compared to a system without visualizations that was similar to a standard playback control. Furthermore, nine (out of ten) participants preferred DemoWiz over the standard playback control with the last expressing no preference.},
 pages = {1581--1590},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/demowiz.pdf},
 primary = {Presentation},
 publisher = {},
 reference = {Chi, Pei-Yu, Bongshin Lee, and Steven M. Drucker. DemoWiz: re-performing software demonstrations for a live presentation. Proceedings of the 32nd annual ACM conference on Human factors in computing systems. ACM, 2014.},
 school = {},
 text = {demowiz.txt},
 thumb = {thumbnail/demowiz.png},
 title = {DemoWiz: re-performing software demonstrations for a live presentation},
 type = {},
 video = {},
 volume = {},
 year = {2014}
}

@article{colburn2000role,
 author = {Colburn, Alex and Cohen, Michael F and Drucker, Steven and others},
 booktitle = {},
 caption = {Avatar Eye Gaze},
 editor = {},
 facet.collaborators = {Colburn,Cohen},
 facet.publication = {InternalReport},
 facet.subject = {Graphics,Animation},
 facet.year = {2000},
 id = {17.0},
 img = {researchImages/eyegaze.jpg},
 institution = {},
 journal = {Sketches and Applications, Siggraph'00},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {As we begin to create synthetic characters (avatars) for computer users, it is important to pay attention to both the look and the behavior of the avatar's eyes. In this paper we present behavior models of eye gaze patterns in the context of real-time verbal communication. We apply these eye gaze models to simulate eye movements in a computer-generated avatar in a number of task settings. We also report the results of an experiment that we conducted to assess whether our eye gaze model induces changes in the eye gaze behavior of an individual who is conversing with an avatar.},
 pages = {},
 pdf = {http://research.microsoft.com/research/pubs/view.aspx?type=Technical%20Report&id=391},
 primary = {Graphics},
 publisher = {},
 reference = {Alex Colburn, Michael F. Cohen, Steven Drucker, The Role of Eye Gaze in Avatar Mediated Conversational Interfaces ,  MSR-TR-2000-81, July, 2000},
 school = {},
 text = {},
 thumb = {thumbnail/eyegaze.jpg},
 title = {The role of eye gaze in avatar mediated conversational interfaces},
 type = {},
 video = {},
 volume = {},
 year = {2000}
}

@article{colburn2001graphical,
 author = {Colburn, R Alex and Cohen, Michael F and Drucker, Steven M and LeeTiernan, Scott and Gupta, Anoop},
 booktitle = {},
 caption = {Graphic enhancement for conference calls},
 editor = {},
 facet.collaborators = {Colburn,Cohen,Counts,Gupta},
 facet.publication = {InternalReport},
 facet.subject = {Graphics,Social},
 facet.year = {2000},
 id = {20.0},
 img = {researchImages/conferencecalls.jpg},
 institution = {},
 journal = {Microsoft Corporation, Redmond, WA, Technical Report MSR-TR-2001-95},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {We present two very low bandwidth graphically enhanced interfaces for small group voice communications. One interface presents static images of the participants that highlight when one is speaking. The other interface utilizes three-dimensional avatars that can be quickly created. Eleven groups of 4 or 5 people were presented with each enhanced interface as well as conducting a live conversation and a voice only conversation. Experiments show that both graphically enhanced interfaces improve the understandability of conversations, particular with respect to impressions that others in the group could express themselves more easily, knowing who is talking, and when to speak. Little difference was found between the two graphical interfaces. Analysis of voice tracks also revealed differences between interfaces in the length and number of medium duration silences.},
 pages = {},
 pdf = {http://research.microsoft.com/research/pubs/view.aspx?type=Technical%20Report&id=499},
 primary = {Social},
 publisher = {},
 reference = {Alex Colburn, Michael F. Cohen, Steven Drucker, Scott Lee-Tiernan, Anoop Gupta, Graphical Enhancement For Voice Only Conference Calls,  MSR-TR-2001-95, 2001.},
 school = {},
 text = {TR2007-04-02.txt},
 thumb = {thumbnail/conferencecalls.jpg},
 title = {Graphical enhancements for voice only conference calls},
 type = {},
 video = {},
 volume = {},
 year = {2001}
}

@techreport{deepnews,
 author = {Steven M. Drucker, Curtis Wong},
 booktitle = {},
 caption = {DeepNews},
 editor = {},
 facet.collaborators = {Wong},
 facet.publication = {InternalReport},
 facet.subject = {UI,TV,Media},
 facet.year = {2001},
 id = {22.0},
 img = {researchImages/deepnews_photo.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {By monitoring the closed caption stream of a news broadcast, the web can be searched for related articles and more in depth stories can be found.},
 pages = {},
 pdf = {},
 primary = {Media},
 publisher = {},
 reference = {Drucker, S.M., Wong, C. 2001, DeepNews: Automatic related material based on closed caption information, MS Technical Report},
 school = {},
 text = {},
 thumb = {thumbnail/deepnews_photo.jpg},
 title = {DeepNews: Automatic related material based on closed caption information},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/DeepnewsEnhanced.wmv},
 volume = {},
 year = {2001}
}

@inproceedings{deline2006code,
 author = {DeLine, Robert and Czerwinski, Mary and Meyers, Brian and Venolia, Gina and Drucker, Steven and Robertson, George},
 booktitle = {Visual Languages and Human-Centric Computing (VL/HCC'06)},
 caption = {Code Thumbnails},
 editor = {},
 facet.collaborators = {DeLine,Czerwinski,Meyers,Venolia,Robertson},
 facet.publication = {VLL/HCC},
 facet.subject = {Visualization,Programming},
 facet.year = {2006},
 id = {41.0},
 img = {researchImages/codethumb.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {IEEE},
 abstract = {Modern development environments provide many features for navigating source code, yet recent studies show the developers still spend a tremendous amount of time just navigating. Since existing navigation features rely heavily on memorizing symbol names, we present a new design, called Code Thumbnails, intended to allow a developer to navigate source code by forming a spa-tial memory of it. To aid intra-file navigation, we add a thumbnail image of the file to the scrollbar, which makes any part of the file one click away. To aid inter-file navigation, we provide a desktop of file thumbnail images, which make any part of any file one click away. We did a formative evaluation of the design with eleven experienced developers and present the results.},
 pages = {11--18},
 pdf = {http://research.microsoft.com/~sdrucker/papers/vlhcc06-final.pdf},
 primary = {Visualization},
 publisher = {},
 reference = {DeLine, R., M. Czerwinski, B. Meyers, G. Venolia, S. Drucker, and G. Robertson.  Code Thumbnails: Using Spatial Memory to Navigate Source Code.  Proc. VL/HCC 2006},
 school = {},
 text = {vdmfinal.txt},
 thumb = {thumbnail/codethumb.jpg},
 title = {Code thumbnails: Using spatial memory to navigate source code},
 type = {},
 video = {},
 volume = {},
 year = {2006}
}

@inproceedings{dontcheva2005v4v,
 author = {Dontcheva, Mira and Drucker, Steven M and Cohen, Michael F},
 booktitle = {Proceedings of the 2005 conference on Designing for User eXperience},
 caption = {V4V},
 editor = {},
 facet.collaborators = {Dontcheva,Cohen},
 facet.publication = {DUX},
 facet.subject = {Animation,UI,Presentation},
 facet.year = {2005},
 id = {36.0},
 img = {researchImages/v4v.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {AIGA: American Institute of Graphic Arts},
 abstract = {We present a View for the Viewer (v4v), a slide viewer that focuses on the needs of the viewer of a presentation instead of the presenter. Our design centers on representing the deck of slides as a stack embedded in a 3-D world. With only single button clicks, the viewer can quickly and easily navigate the deck of slides. We provide four types of annotation techniques and have designed a synchronization mechanism that makes it easy for the viewer to move in and out of sync with the presenter. We also supply alarms as a method for viewer notification. We evaluate our approach with a preliminary user study resulting in positive feedback about our design plus suggestions for improvements and extensions.},
 pages = {19},
 pdf = {http://research.microsoft.com/~sdrucker/papers/v4v.pdf},
 primary = {Presentation},
 publisher = {},
 reference = {Dontcheva M., Drucker S., Cohen M., v4v: a View for the Viewer, DUX 2005},
 school = {},
 text = {v4v.txt},
 thumb = {thumbnail/v4v.jpg},
 title = {v4v: a View for the Viewer},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/v4v.avi},
 volume = {},
 year = {2005}
}

@inproceedings{dontcheva2006collecting,
 author = {Dontcheva, Mira and Drucker, Steven M and Wade, Geraldine and Salesin, David and Cohen, Michael F},
 booktitle = {Personal Information Management-Special Interest Group for Information Retrieval Workshop},
 caption = {Collecting Web Content},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {},
 pages = {44--47},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Collecting and organizing web content},
 type = {},
 video = {},
 volume = {},
 year = {2006}
}

@inproceedings{dontcheva2006summarizing,
 author = {Dontcheva, Mira and Drucker, Steven M and Wade, Geraldine and Salesin, David and Cohen, Michael F},
 booktitle = {Proceedings of the 19th annual ACM symposium on User interface software and technology},
 caption = {Summarizing Web Sessions},
 editor = {},
 facet.collaborators = {Dontcheva,Salesin,Wade,Cohen},
 facet.publication = {UIST},
 facet.subject = {Visualization,UI,Web,Search},
 facet.year = {2006},
 id = {40.0},
 img = {researchImages/webpage_photo.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {We describe a system, implemented as a browser extension, that enables users to quickly and easily collect, view, and share personal Web content. Our system employs a novel interaction model, which allows a user to specify webpage extraction patterns by interactively selecting webpage elements and applying these patterns to automatically collect similar content. Further, we present a technique for creating visual summaries of the collected information by combining user labeling with predefined layout templates. These summaries are interactive in nature: depending on the behaviors encoded in their templates, they may respond to mouse events, in addition to providing a visual summary. Finally, the summaries can be saved or sent to others to continue the research at another place or time. Informal evaluation shows that our approach works well for popular websites, and that users can quickly learn this interaction model for collecting content from the Web.},
 pages = {115--124},
 pdf = {http://research.microsoft.com/~sdrucker/papers/uistPaperSummarizing.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Dontcheva, M., Drucker, S. M., Wade, G., Salesin, D., and Cohen, M. F. 2006. Summarizing personal web browsing sessions. In Proceedings of the 19th Annual ACM Symposium on User interface Software and Technology (Montreux, Switzerland, October 15 - 18, 2006). UIST '06. ACM Press, New York, NY, 115-124.},
 school = {},
 text = {uistPaperSummarizing.txt},
 thumb = {thumbnail/webpage_photo.gif},
 title = {Summarizing personal web browsing sessions},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/uistSummariesfinalCut.mov},
 volume = {},
 year = {2006}
}

@inproceedings{dontcheva2007relations,
 author = {Dontcheva, Mira and Drucker, Steven M and Salesin, David and Cohen, Michael F},
 booktitle = {Proceedings of the 20th annual ACM symposium on User interface software and technology},
 caption = {Relations, Templates},
 editor = {},
 facet.collaborators = {Dontcheva,Cohen,Salesin},
 facet.publication = {UIST},
 facet.subject = {Visualization,UI,Search},
 facet.year = {2007},
 id = {44.0},
 img = {researchImages/relations.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {in collecting and organizing Web content. First, we demonstrate an interface for creating associations between websites, which facilitate the automatic retrieval of related content. Second, we present an authoring interface that allows users to quickly merge content from many different websites into a uniform and personalized representation, which we call a card. Finally, we introduce a novel search paradigm that leverages the relationships in a card to direct search queries to extract relevant content from multipleWeb sources and fill a new series of cards instead of just returning a list of webpage URLs. Preliminary feedback from users is positive and validates our design},
 pages = {61--70},
 pdf = {http://research.microsoft.com/~sdrucker/papers/DontchevaUist07.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Dontcheva, M, Drucker, S.M., Cohen, M., Salesin, D. Relations, Cards, and Search Templates: User-guided Web Data Integration and Layout, To Appear in UIST, 2007},
 school = {},
 text = {DontchevaUist07.txt},
 thumb = {thumbnail/relations.png},
 title = {Relations, cards, and search templates: user-guided web data integration and layout},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/DontchevaUist07.mov},
 volume = {},
 year = {2007}
}

@inproceedings{dontcheva2008experiences,
 author = {Dontcheva, Mira and Lin, Sharon and Drucker, Steven M and Salesin, David and Cohen, Michael F},
 booktitle = {Proc SUI},
 caption = {Content Extraction},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {},
 pages = {},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Experiences with content extraction from the web},
 type = {},
 video = {},
 volume = {},
 year = {2008}
}

@article{dontcheva2010web,
 author = {Dontcheva, Mira and Drucker, Steven M and Salesin, David and Cohen, Michael F},
 booktitle = {},
 caption = {},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {No Code Required: Giving Users Tools to Transform the Web},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {},
 pages = {235},
 pdf = {},
 primary = {},
 publisher = {Morgan Kaufmann},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {From Web Summaries to search templates},
 type = {},
 video = {},
 volume = {},
 year = {2010}
}

@inproceedings{drucker1992cinema,
 author = {Drucker, Steven M and Galyean, Tinsley A and Zeltzer, David},
 booktitle = {Proceedings of the 1992 symposium on Interactive 3D graphics},
 caption = {Cinema Program},
 editor = {},
 facet.collaborators = {Galyean,Zeltzer},
 facet.publication = {SIGGRAPH,Thesis},
 facet.subject = {Graphics,Camera,Thesis},
 facet.year = {1992},
 id = {5.0},
 img = {researchImages/cinema.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {This paper presents a general system for camera movement upon which a wide variety of higher-level methods and applications can be built. In addition to the basic commands for camera placement, a key attribute of the CINEMA system is the ability to inquire information directly about the 3D world through which the camera is moving. With this information high-level procedures can be written that closely correspond to more natural camera specifications. Examples of some high-level procedures are presented. In addition, methods for overcoming deficiencies of this procedural approach are proposed.},
 pages = {67--70},
 pdf = {http://research.microsoft.com/~sdrucker/papers/SIG92symp.pdf},
 primary = {Camera},
 publisher = {},
 reference = {Drucker, S.M., Galyean, T.A., and Zeltzer, D. CINEMA: A System for Procedural Camera Movements. SIGGRAPH Symposium on 3D Interaction. Cambridge, MA. 1992.},
 school = {},
 text = {SIG92symp.txt},
 thumb = {thumbnail/cinema.gif},
 title = {Cinema: A system for procedural camera movements},
 type = {},
 video = {},
 volume = {},
 year = {1992}
}

@inproceedings{drucker1992fast,
 author = {Drucker, Steven M and Schr{\"o}der, Peter},
 booktitle = {Third Eurographics Workshop on Rendering (Bristol, uk},
 caption = {Parallel Radiosity},
 editor = {},
 facet.collaborators = {Schroeder},
 facet.publication = {PhotorealisticWorkshop},
 facet.subject = {Graphics,Parallel Computing},
 facet.year = {1992},
 id = {8.0},
 img = {researchImages/parallelRadiosity.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {We present a data parallel algorithm for radiosity. The algorithm was designed to take advantage of large numbers of processors. It has been implemented on the Connection Machine CM2 system and scales linearly in the number of available processors over a wide range. All parts of the algorithm | form-factor computation, visibility determination, adaptive subdivision, and linear algebra solution | execute in parallel with a completely distributed database. Load balancing is achieved through processor allocation and dynamic data structures which reconfigure appropriately to match the granularity of the required calculations.},
 pages = {247--258},
 pdf = {http://research.microsoft.com/~sdrucker/papers/eurorendworkshop.pdf},
 primary = {Graphics},
 publisher = {},
 reference = {Drucker, S.M. and Schroeder, P. Fast Radiosity:A Data Parallel Approach. 3rd Workshop on Photorealistic Rendering, Bristol, U.K. 1992.},
 school = {},
 text = {eurorendworkshop.txt},
 thumb = {thumbnail/parallelRadiosity.gif},
 title = {Fast radiosity using a data parallel architecture},
 type = {},
 video = {},
 volume = {},
 year = {1992}
}

@phdthesis{drucker1994intelligent,
 author = {Drucker, Steven Mark},
 booktitle = {},
 caption = {Conversation Agent},
 editor = {},
 facet.collaborators = {Individual},
 facet.publication = {Camera},
 facet.subject = {Graphics,Animation,Camera,Thesis},
 facet.year = {1994},
 id = {9.0},
 img = {researchImages/conversesm.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Too often in the field of computer graphics, practitioners have been more concerned with the question of how to move a camera rather than why to move it. This thesis addresses the core question of why the camera is being placed and moved and uses answers to that question to provide a more convenient, more intelligent method for controlling virtual cameras in computer graphics. After discussing the general sorts of activities to be performed in graphical environments, this thesis then contains a derivation of some camera primitives that are required, and examines how they can be incorporated into different interfaces. A single, consistent, underlying framework for camera control across many different domains has been posited and formulated in terms of constrained optimization. Examples from different application domains demonstrate a variety of interface styles that have all been implemented on top of the underlying framework. Evaluations for each application are also given.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/thesiswbmakrs.pdf},
 primary = {Camera},
 publisher = {},
 reference = {Drucker, S.M. Intelligent Camera Control for Graphical Environments PhD Thesis, MIT Media Lab. 1994.},
 school = {Massachusetts Institute of Technology},
 text = {thesiswbmakrs.txt},
 thumb = {thumbnail/conversesm.gif},
 title = {Intelligent camera control for graphical environments},
 type = {},
 video = {},
 volume = {},
 year = {1994}
}

@inproceedings{drucker1994intelligent,
 author = {Drucker, Steven M and Zeltzer, David},
 booktitle = {Graphics Interface},
 caption = {Virtual Museum},
 editor = {},
 facet.collaborators = {Zeltzer},
 facet.publication = {GI,Thesis},
 facet.subject = {Graphics,Camera,Thesis},
 facet.year = {1994},
 id = {11.0},
 img = {researchImages/museum1.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {CANADIAN INFORMATION PROCESSING SOCIETY},
 abstract = {This paper describes a framework for exploring intelligent camera controls in a 3D virtual environment. It presents a methodology for designing the underlying camera controls based on an analysis of what tasks are to be required in a specific environment. Once an underlying camera framework is built, a variety of interfaces can be connected to the framework. A virtual museum is used as a prototypical virtual environment for this work. This paper identifies some of the tasks that need to be performed in a virtual museum; presents a paradigm for encapsulating those tasks into camera modules; and describes in detail the underlying mechanisms that make up the camera module for navigating through the environment.},
 pages = {190--190},
 pdf = {http://research.microsoft.com/~sdrucker/papers/GImuseum_wfigs.pdf},
 primary = {Camera},
 publisher = {},
 reference = {Drucker, S.M. and Zeltzer, D. Intelligent Camera Control in a Virtual Environment Graphics Interface '94.},
 school = {},
 text = {GImuseum_wfigs.txt},
 thumb = {thumbnail/museum1.gif},
 title = {Intelligent camera control in a virtual environment},
 type = {},
 video = {},
 volume = {},
 year = {1994}
}

@inproceedings{drucker2002smartskip,
 author = {Drucker, Steven M and Glatzer, Asta and De Mar, Steven and Wong, Curtis},
 booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems},
 caption = {Smart Skip},
 editor = {},
 facet.collaborators = {Roseway,De Mar,Wong},
 facet.publication = {SIGCHI},
 facet.subject = {UI,TV,Media},
 facet.year = {2002},
 id = {24.0},
 img = {researchImages/smartskip_photo.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {In this paper, we describe an interface for browsing and skipping digital video content in a consumer setting; that is, sitting and watching television from a couch using a standard remote control. We compare this interface with two other interfaces that are in common use today and found that subjective satisfaction was statistically better with the new interface. Performance metrics however, like time to task completion and number of clicks were worse.},
 pages = {219--226},
 pdf = {http://research.microsoft.com/~sdrucker/papers/smartskipfinal.pdf},
 primary = {Media},
 publisher = {},
 reference = {Drucker, S.,  Glatzer, A., De Mar, S and Wong, C. SmartSkip: Consumer level browsing and skipping of digital video content. In Proceedings of CHI 2002, Minneapolis, Minnesota, 2002},
 school = {},
 text = {smartskipfinal.txt},
 thumb = {thumbnail/smartskip_photo.jpg},
 title = {SmartSkip: consumer level browsing and skipping of digital video content},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/smartskip2.wmv},
 volume = {},
 year = {2002}
}

@techreport{drucker2002spectator,
 author = {Drucker, Steven and He, Li-wei and Cohen, Michael and Wong, Curtis and Gupta, Anoop},
 booktitle = {},
 caption = {Spectator (concept)},
 editor = {},
 facet.collaborators = {He,Cohen,Gupta,Wong,Roseway,De Mar},
 facet.publication = {InternalReport},
 facet.subject = {Graphics,UI,Games},
 facet.year = {2003},
 id = {31.0},
 img = {researchImages/spectator_photo.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Networked multiplayer games are becoming tremendously popular. At any given moment on the Microsoft Game Zone (http://zone.msn.com), there are thousands of people playing Asheron's Call or Age of Empires. Traditional board and card games are also increasingly being played online and will continue to gain in popularity. While networked games are certainly fun for active players, there is potentially a much larger audience: spectators. In most traditional games, such as football, the number of spectators far exceeds the number of players. The key idea presented in this paper is to tap this potential by making online games engaging and entertaining to non-players watching these games. <br> The experience for spectators can be made much richer by employing techniques often used in sports broadcasting, such as a commentator providing analysis and background stories, slow motion and instance replay. For 3D games, cinematic camera movements and shot cuts be much more visually interesting than the first-person views often provided to the players. There is the potential to significantly increase the 'eyeballs' on sites such as Microsoft Game Zone. Spectators can be more easily targeted for advertising. Finally, supporting the spectator experience will help drive sales of the games themselves as casual viewers take the next step to become players. Watching others play networked games has the potential to become a vital component to an overall entertainment/media strategy. The authors of this document have already developed significant technologies needed to support the online game spectator. We propose that new resources be devoted now to carry these technologies into practice.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/spectator.pdf},
 primary = {Camera},
 publisher = {},
 reference = {Drucker, S.M., He. L, Cohen, M., Gupta., A, Wong, C., Spectator Games: A New Entertainment Modality for Networked Multiplayer Games, 2003.},
 school = {},
 text = {},
 thumb = {thumbnail/spectator_photo.jpg},
 title = {Spectator games: A new entertainment modality of networked multiplayer games},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/spectator.asf},
 volume = {},
 year = {2002}
}

@techreport{drucker2003photo,
 author = {Drucker, Steven and Wong, Curtis and Roseway, Asta and Glenner, Steve and De Mar, Steve},
 booktitle = {},
 caption = {Phototriage},
 editor = {},
 facet.collaborators = {Wong,Roseway,Glenner,De Mar},
 facet.publication = {InternalReport},
 facet.subject = {UI,Photos},
 facet.year = {2003},
 id = {28.0},
 img = {researchImages/phototriage.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {MSR-TR-2003-99},
 organization = {},
 abstract = {The Photo-triage application is meant to be an essential part of the digital photography lifestyle. It can fit as a component wherever photo management is done (shell, picture-it, media-center, etc.). The central idea is to facilitate rapid, convenient categorization of one's personal photos into at least the following categories: hidden/private, majority, highlights, best and/or representative. See figure 1. This application is meant to fill an empty niche in the usage of digital photos: that is, there's no easy way add metadata to photos to mark them for printing, for sharing, or for slideshows without creating separate versions of the photos and copying into separate folders. We propose a sorting metaphor that will add implicit metadata when one first goes through the photos. },
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/phototriage.pdf},
 primary = {Photos},
 publisher = {},
 reference = {Drucker, S. C. Wong, A. Roseway, S. Glenner, S. De Mar, Photo-triage: Rapidly annotating your digital photographs. MSR Tech Report.},
 school = {},
 text = {phototriage.txt},
 thumb = {thumbnail/phototriage.jpg},
 title = {Photo-triage: Rapidly annotating your digital photographs},
 type = {},
 video = {},
 volume = {},
 year = {2003}
}

@inproceedings{drucker2004mediabrowser,
 author = {Drucker, Steven M and Wong, Curtis and Roseway, Asta and Glenner, Steven and De Mar, Steven},
 booktitle = {Proceedings of the working conference on Advanced visual interfaces},
 caption = {MediaBrowser},
 editor = {},
 facet.collaborators = {Wong,Roseway,Glenner,De Mar},
 facet.publication = {AVI},
 facet.subject = {Graphics,Photos,UI,Visualization},
 facet.year = {2004},
 id = {34.0},
 img = {researchImages/MFAG.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Applying personal keywords to images and video clips makes it possible to organize and retrieve them, as well as automatically create thematically related slideshows. MediaBrowser is a system designed to help users create annotations by uniting a careful choice of interface elements, an elegant and pleasing design, smooth motion and animation, and a few simple tools that are predictable and consistent. The result is a friendly, useable tool for turning shoeboxes of old photos into labeled collections that can be easily browsed, shared, and enjoyed.},
 pages = {433--436},
 pdf = {http://research.microsoft.com/~sdrucker/papers/mediaframeAVIlong.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Drucker, S. C. Wong, A. Roseway, S. Glenner, S. De Mar, MediaBrowser: Reclaiming the Shoebox. in Proceedings of AVI2004, Gallipoli, Italy, 2004.},
 school = {},
 text = {mediaframeAVIlong.txt},
 thumb = {thumbnail/MFAG.png},
 title = {MediaBrowser: reclaiming the shoebox},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/LH%20MediaFrame%20Final.wmv},
 volume = {},
 year = {2004}
}

@inproceedings{drucker2005visual,
 author = {Drucker, Steven M and Regan, Tim and Roseway, Asta and Lofstrom, Markus},
 booktitle = {Proceedings of the 2005 conference on Designing for User eXperience},
 caption = {Visual Decision Maker},
 editor = {},
 facet.collaborators = {Regan,Roseway,Lofstrom},
 facet.publication = {DUX},
 facet.subject = {Graphics,UI,Movies,Visualization},
 facet.year = {2005},
 id = {37.0},
 img = {researchImages/VDM.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {AIGA: American Institute of Graphic Arts},
 abstract = {We present the Visual Decision Maker (VDM), an application that gives movie recommendations to groups of people sitting together. The VDM provides a TV like user experience: a stream of movie stills flows towards the center of the screen, and users press buttons on remote controls to vote on the currently selected movie. A collaborative filtering engine provides recommendations for each user and for the group as a whole based on the votes. Three principles guided our design of the VDM: shared focus, dynamic pacing, and encouraging conversations. In this paper we present the results of a four month public installation and a lab study showing how these design choices affected people's usage and people's experience of the VDM. Our results show that shared focus is important for users to feel that the group's tastes are represented in the recommendations.},
 pages = {21},
 pdf = {http://research.microsoft.com/~sdrucker/papers/vdmfinal.pdf},
 primary = {Media},
 publisher = {},
 reference = {Drucker, S., Regan, T., Roseway, A., Lofstrom. M, The visual decision maker: a recommendation system for collocated users, DUX 2005},
 school = {},
 text = {vdmfinal.txt},
 thumb = {thumbnail/VDM.jpg},
 title = {The visual decision maker: A recommendation system for collocated users},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/vdm.wmv},
 volume = {},
 year = {2005}
}

@inproceedings{drucker2006comparing,
 author = {Drucker, Steven M and Petschnigg, Georg and Agrawala, Maneesh},
 booktitle = {Proceedings of the 19th annual ACM symposium on User interface software and technology},
 caption = {Powerpoint Diff},
 editor = {},
 facet.collaborators = {Petschnigg,Agrawala},
 facet.publication = {UIST},
 facet.subject = {Visualization,UI,Presentation,Temporal},
 facet.year = {2006},
 id = {39.0},
 img = {researchImages/vizpptdiff.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Despite the ubiquity of slide presentations, managing multiple presentations remains a challenge. Understanding how multiple versions of a presentation are related to one another, assembling new presentations from existing presentations, and collaborating to create and edit presentations are difficult tasks. <br>  In this paper, we explore techniques for comparing and managing multiple slide presentations. We propose a general comparison framework for computing similarities and differences between slides. Based on this framework we develop an interactive tool for visually comparing multiple presentations. The interactive visualization facilitates understanding how presentations have evolved over time. We show how the interactive tool can be used to assemble new presentations from a collection of older ones and to merge changes from multiple presentation authors.},
 pages = {47--56},
 pdf = {http://research.microsoft.com/~sdrucker/papers/fp214-DruckerFinalSmall.pdf},
 primary = {Visualization},
 publisher = {},
 reference = {Drucker, S. M., Petschnigg, G., and Agrawala, M. 2006. Comparing and managing multiple versions of slide presentations. In Proceedings of the 19th Annual ACM Symposium on User interface Software and Technology (Montreux, Switzerland, October 15 - 18, 2006). UIST '06. ACM Press, New York, NY, 47-56.},
 school = {},
 text = {fp214-DruckerFinalSmall.txt},
 thumb = {thumbnail/vizpptdiff.jpg},
 title = {Comparing and managing multiple versions of slide presentations},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/pptviznew.wmv},
 volume = {},
 year = {2006}
}

@inproceedings{drucker2011helping,
 author = {Drucker, Steven M and Fisher, Danyel and Basu, Sumit},
 booktitle = {IFIP Conference on Human-Computer Interaction},
 caption = {iCluster},
 editor = {},
 facet.collaborators = {Fisher,Basu},
 facet.publication = {Interact},
 facet.subject = {UI,Visualization,Machine Learning},
 facet.year = {2011},
 id = {60.0},
 img = {researchImages/iclustering1.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {Springer Berlin Heidelberg},
 abstract = {Sorting and clustering large numbers of documents can be an overwhelming task: manual solutions tend to be slow, while machine learning systems often present results that don???t align well with users' intents. We created and evaluated a system for helping users sort large numbers of documents into clusters. iCluster has the capability to recommend new items for existing clusters and appropriate clusters for items. The recommendations are based on a learning model that adapts over time ??? as the user adds more items to a cluster, the system???s model improves and the recommendations become more relevant. Thirty-two subjects used iCluster to sort hundreds of data items both with and without recommendations; we found that recommendations allow users to sort items more rapidly. A pool of 161 raters then assessed the quality of the resulting clusters, finding that clusters generated with recommendations were of statistically indistinguishable quality. Both the manual and assisted methods were substantially better than a fully automatic method.},
 pages = {187--203},
 pdf = {http://research.microsoft.com/~sdrucker/papers/icluster_nonanonymous_sdrucker_cameraready.pdf},
 primary = {Visualization},
 publisher = {},
 reference = {Steven M. Drucker, Danyel Fisher, and Sumit Basu, Helping Users Sort Faster with Adaptive Machine Learning Recommendations, in Proceedings of Interact 2011, Springer, September 2011},
 school = {},
 text = {icluster_nonanonymous_sdrucker_cameraready.txt},
 thumb = {thumbnail/iclustering1.png},
 title = {Helping users sort faster with adaptive machine learning recommendations},
 type = {},
 video = {http://www.youtube.com/watch?feature=player_embedded&v=3BmO3TILucQ},
 volume = {},
 year = {2011}
}

@inproceedings{drucker2013touchviz,
 author = {Drucker, Steven M and Fisher, Danyel and Sadana, Ramik and Herron, Jessica and others},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 caption = {TouchViz},
 editor = {},
 facet.collaborators = {Fisher,Sadana,Herron,schraefel},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Information,Visualization,Touch},
 facet.year = {2013},
 id = {73.0},
 img = {researchImages/touchvis.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM}, 
 abstract = {As more applications move from the desktop to touch devices like tablets, designers must wrestle with the costs of porting a design with as little revision of the UI as possible from one device to the other, or of optimizing the interaction per device. We consider the tradeoffs between two versions of a UI for working with data on a touch tablet. One interface is based on using the conventional desktop metaphor (WIMP) with a control panel, push buttons, and checkboxes where the mouse click is effectively replaced by a finger tap. The other interface (which we call FLUID) eliminates the control panel and focuses touch actions on the data visualization itself. We describe our design process and evaluation of each interface. We discuss the significantly better task performance and preference for the FLUID interface, in particular how touch design may challenge certain assumptions about the performance benefits of WIMP interfaces that do not hold on touch devices, such as the superiority of gestural vs. control panel based interaction.},
 pages = {2301--2310},
 pdf = {http://research.microsoft.com/~sdrucker/papers/touchvis-CHI2013-cameraready.pdf},
 primary = {Visualization},
 publisher = {},
 reference = {Steven M. Drucker, Danyel Fisher, Ramik Sadana, Jessica Herron, and mc schraefel, TouchViz, A Case Study Comparing Tow Interfaces for Data Analytics on Tablets, in Proceedings of the 2012 Conference on Human Factors in Computing Systems (CHI 2013), ACM Conference on Human Factors in Computing Systems, 29 April 2013},
 school = {},
 text = {touchvis-CHI2013-cameraready.txt},
 thumb = {thumbnail/touchvis.png},
 title = {TouchViz: a case study comparing two interfaces for data analytics on tablets},
 type = {},
 video = {http://research.microsoft.com/en-us/um/people/sdrucker/video/panodata.mp4.mp4},
 volume = {},
 year = {2013}
}

@misc{drucker22002spectator2,
 author = {Steven M. Drucker, Steven De Mar},
 booktitle = {},
 caption = {Spectator (MechWarrior)},
 editor = {},
 facet.collaborators = {He,Cohen,Gupta,Wong,De Mar},
 facet.publication = {InternalReport},
 facet.subject = {Graphics,UI,Games,Thesis},
 facet.year = {2003},
 id = {32.0},
 img = {researchImages/spectator3.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Networked multiplayer games are becoming tremendously popular. At any given moment on the Microsoft Game Zone (http://zone.msn.com), there are thousands of people playing Asheron's Call or Age of Empires. Traditional board and card games are also increasingly being played online and will continue to gain in popularity. While networked games are certainly fun for active players, there is potentially a much larger audience: spectators. In most traditional games, such as football, the number of spectators far exceeds the number of players. The key idea presented in this paper is to tap this potential by making online games engaging and entertaining to non-players watching these games. <br> The experience for spectators can be made much richer by employing techniques often used in sports broadcasting, such as a commentator providing analysis and background stories, slow motion and instance replay. For 3D games, cinematic camera movements and shot cuts be much more visually interesting than the first-person views often provided to the players. There is the potential to significantly increase the 'eyeballs' on sites such as Microsoft Game Zone. Spectators can be more easily targeted for advertising. Finally, supporting the spectator experience will help drive sales of the games themselves as casual viewers take the next step to become players. Watching others play networked games has the potential to become a vital component to an overall entertainment/media strategy. The authors of this document have already developed significant technologies needed to support the online game spectator. We propose that new resources be devoted now to carry these technologies into practice.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/spectator.pdf},
 primary = {Camera},
 publisher = {},
 reference = {Drucker, S.M., He. L, Cohen, M., Gupta., A, Wong, C., Spectator Games: A New Entertainment Modality for Networked Multiplayer Games, 2003.},
 school = {},
 text = {spectator.txt},
 thumb = {thumbnail/spectator3.jpg},
 title = {Spectator implementation in Mechwarrior},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/spectator3.wmv},
 volume = {},
 year = {2002}
}

@techreport{druckermoving,
 author = {Drucker, Steven M},
 booktitle = {},
 caption = {MOOs to Multi-user apps},
 editor = {},
 facet.collaborators = {Individual},
 facet.publication = {InternalReport},
 facet.subject = {Graphics,Social},
 facet.year = {1999},
 id = {16.0},
 img = {researchImages/uistmoos.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {This paper provides a brief description of the work we have done on the V-Worlds project, a system that facilitates the creation of multi-user applications and environments. We have taken concepts originally found in object oriented Multi-User Dungeons (MOOs) and extended them to deal with more general multi-user and in particular multi-media applications. We present reasons behind the architectural decisions of the platform and show that it has been used successfully for a wide range of examples.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/moving.pdf},
 primary = {Social},
 publisher = {},
 reference = {Steven M. Drucker, Moving from MOOs to Multi-user Applications,  Internal Report, 1999.},
 school = {},
 text = {moving.txt},
 thumb = {thumbnail/uistmoos.jpg},
 title = {Moving from MOOs to Multi-User Applications},
 type = {},
 video = {},
 volume = {},
 year = {1999}
}

@article{edge2016slidespace,
 author = {Edge, Darren and Yang, Xi and Kotturi, Yasmine and Wang, Shuoping and Feng, Dan and Lee, Bongshin and Drucker, Steven},
 booktitle = {},
 caption = {SlideSpace},
 editor = {},
 facet.collaborators = {Edge,Yang,Kotturi,Wang,Feng,Lee},
 facet.publication = {TOCHI},
 facet.subject = {UI,Presentation,Information},
 facet.year = {2016},
 id = {84.0},
 img = {researchImages/slidespace.png},
 institution = {},
 journal = {ACM Transactions on Computer-Human Interaction (TOCHI)},
 link = {},
 month = {},
 number = {3},
 organization = {},
 abstract = {The Slide and Canvas metaphors are two ways of helping people create visual aids for oral presentations. Although such physical metaphors help both authors and audiences make sense of material, they also constrain authoring in ways that can negatively impact presentation delivery. In this article, we derive heuristics for the design of presentation media that are independent of any underlying physical metaphors. We use these heuristics to craft a new kind of presentation medium called SlideSpace?one that combines hierarchical outlines, content collections, and design rules to automate the real-time, outline-driven synthesis of hybrid Slide-Canvas visuals. Through a qualitative study of SlideSpace use, we validate our heuristics and demonstrate that such a hybrid presentation medium can combine the advantages of existing systems while mitigating their drawbacks. Overall, we show how a heuristic design approach helped us challenge entrenched physical metaphors to create a fundamentally digital presentation medium with the potential to transform the activities of authoring, delivering, and viewing presentations.},
 pages = {16},
 pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/a16-edge.pdf},
 primary = {Presentation},
 publisher = {ACM},
 reference = {Edge, Darren, et al. "SlideSpace: Heuristic Design of a Hybrid Presentation Medium."?ACM Transactions on Computer-Human Interaction (TOCHI)?23.3 (2016): 16.},
 school = {},
 text = {},
 thumb = {thumbnail/slidespace.png},
 title = {SlideSpace: Heuristic Design of a Hybrid Presentation Medium},
 type = {},
 video = {},
 volume = {23},
 year = {2016}
}

@article{edmonds2007instrumenting,
 author = {Edmonds, Andy and White, Ryen W and Morris, Dan and Drucker, Steven M},
 booktitle = {},
 caption = {},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {Journal of Web Engineering},
 link = {},
 month = {},
 number = {3},
 organization = {},
 abstract = {},
 pages = {243},
 pdf = {},
 primary = {},
 publisher = {Rinton Press},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Instrumenting the dynamic web},
 type = {},
 video = {},
 volume = {6},
 year = {2007}
}

@techreport{filteredepg,
 author = {Steven M. Drucker, Curtis Wong},
 booktitle = {},
 caption = {Filtered EPG},
 editor = {},
 facet.collaborators = {Wong,Roseway},
 facet.publication = {InternalReport},
 facet.subject = {UI,TV,Media},
 facet.year = {2002},
 id = {26.0},
 img = {researchImages/tvnow_photo.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {This electronic program guide uses automatically computed favorites based on viewing habits per time of day and day of week as well as simple filtering features to allow for rapid selection of television.},
 pages = {},
 pdf = {},
 primary = {Media},
 publisher = {},
 reference = {Drucker, S.M. 2002, Filtered Electronic Program Guides, MS Technical Report},
 school = {},
 text = {},
 thumb = {thumbnail/tvnow_photo.jpg},
 title = {Filtered Electronic Program Guides},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/nextmedia/TVNow.exe},
 volume = {},
 year = {2002}
}

@article{fisher2010visualizations,
 author = {Fisher, Danyel and Drucker, Steven and Fernandez, Roland and Ruble, Scott},
 booktitle = {},
 caption = {WebCharts},
 editor = {},
 facet.collaborators = {Fisher,Fernandez,Ruble},
 facet.publication = {Infovis},
 facet.subject = {UI,Visualization},
 facet.year = {2010},
 id = {55.0},
 img = {researchImages/webcharts.png},
 institution = {},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 link = {},
 month = {},
 number = {6},
 organization = {},
 abstract = {In order to use new visualizations, most toolkits require application developers to rebuild their applications and distribute new versions to users. The WebCharts Framework take a different approach by hosting Javascript from within an application and providing a standard data and events interchange.. In this way, applications can be extended dynamically, with a wide variety of visualizations. We discuss the benefits of this architectural approach, contrast it to existing techniques, and give a variety of examples and extensions of the basic system.},
 pages = {1157--1163},
 pdf = {http://research.microsoft.com/~sdrucker/papers/webcharts.pdf},
 primary = {Visualization},
 publisher = {IEEE},
 reference = {Danyel Fisher, Steven Drucker, Roland Fernandez, and Scott Ruble, Visualizations Everywhere: A Multiplatform Infrastructure for Linked Visualizations, in Transactions on Visualization and Computer Graphics, IEEE, Salt Lake City, UT, November 2010},
 school = {},
 text = {webcharts.txt},
 thumb = {thumbnail/webcharts.png},
 title = {Visualizations everywhere: A multiplatform infrastructure for linked visualizations},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/icluster_INTERACT11.mov},
 volume = {16},
 year = {2010}
}

@techreport{fisher2011vis,
 author = {Fisher, Danyel and Drucker, Steven and Fernandez, Roland and Chen, Xiaoji},
 booktitle = {},
 caption = {Vis-a-vis},
 editor = {},
 facet.collaborators = {Fisher,Fernandez,Chen},
 facet.publication = {InternalReport},
 facet.subject = {UI,Information,Visualization,Design},
 facet.year = {2011},
 id = {59.0},
 img = {researchImages/visavis.png},
 institution = {Technical Report MSR-TR-2011-142, Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Finding ways for information workers to easily create and modify visualizations that display their own data has been a long time goal within the visualization community. We describe Vis-a-vis, a declarative language for defining and extending visualizations directly within spreadsheets. Vis-a-vis allows users to directly bind data and formula to the visual attributes of an extensible set of visualization primitives. The visualizations that Vis-a-vis creates can be shared and modified easily, allowing users to modify existing visualizations. This approach allows users to select visualizations from a gallery, to customize them easily, or to create novel visualizations. The approach leverages familiar formulas and data from spreadsheets. We prototype a system that uses this language, and use it to build a number of standard and custom visualizations, and gather formative feedback from a small user study.},
 pages = {},
 pdf = {http://research.microsoft.com/apps/pubs/default.aspx?id=159225},
 primary = {Visualization},
 publisher = {},
 reference = {Danyel Fisher, Steven Drucker, Roland Fernandez, and Xiaoji Chen, Vis-a-vis: A Visual Language for Spreadsheet Visualizations, no. MSR-TR-2011-142, June 2011},
 school = {},
 text = {VisAVis.txt},
 thumb = {thumbnail/visavis.png},
 title = {Vis-a-vis: A Visual Language for Spreadsheet Visualizations},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/icluster_INTERACT11.mov},
 volume = {},
 year = {2011}
}

@article{fisher2012exploratory,
 author = {Fisher, Danyel and Drucker, Steven M and K{\"o}nig, Arnd Christian},
 booktitle = {},
 caption = {IncrementalVis},
 editor = {},
 facet.collaborators = {Fisher,Konig},
 facet.publication = {CGA},
 facet.subject = {Visualization},
 facet.year = {2013},
 id = {69.0},
 img = {researchImages/incrvis.png},
 institution = {},
 journal = {IEEE computer graphics and applications},
 link = {},
 month = {},
 number = {4},
 organization = {},
 abstract = {Large datasets can mean slow queries, for which users must wait. Incremental visualization systems can give faster results at a cost of accuracy. This article asked analysts to use one and report on their results. Their feedback provides suggestions for alternative visualizations to represent a query still in progress.},
 pages = {55--62},
 pdf = {http://research.microsoft.com/apps/pubs/default.aspx?id=208568},
 primary = {Visualization},
 publisher = {},
 reference = {Danyel Fisher, Steven M. Drucker, and A. Christian K??nig, Exploratory Visualization Involving Incremental, Approximate Database Queries and Uncertainty, in IEEE Computer Graphics and Applications, IEEE, July 2012},
 school = {},
 text = {Exploratory_CGA.txt},
 thumb = {thumbnail/incrvis.png},
 title = {Exploratory visualization involving incremental, approximate database queries and uncertainty},
 type = {},
 video = {http://research.microsoft.com/pubs/194060/stat-sigmod2013-demo.pdf},
 volume = {32},
 year = {2012}
}

@article{fisher2012interactions,
 author = {Fisher, Danyel and DeLine, Rob and Czerwinski, Mary and Drucker, Steven},
 booktitle = {},
 caption = {Big Data Interaction},
 editor = {},
 facet.collaborators = {Fisher,DeLine,Czerwinski},
 facet.publication = {Interactions},
 facet.subject = {UI,Information,Visualization,Big Data},
 facet.year = {2012},
 id = {66.0},
 img = {researchImages/bigdata.png},
 institution = {},
 journal = {interactions},
 link = {},
 month = {},
 number = {3},
 organization = {},
 abstract = {Increasingly in the 21st century,  our daily lives leave behind a  detailed digital record: our shifting  thoughts and opinions shared on  Twitter, our social relationships, our purchasing habits, our information seeking, our photos and videos - even the movements of our bodies and cars},
 pages = {50--59},
 pdf = {http://research.microsoft.com/apps/pubs/default.aspx?id=163593},
 primary = {Visualization},
 publisher = {ACM},
 reference = {Danyel Fisher, Rob DeLine, Mary Czerwinski, and Steven Drucker, Interactions with Big Data Analytics, in ACM Interactions, ACM, May 2012},
 school = {},
 text = {inteactions_big_data.txt},
 thumb = {thumbnail/bigdata.png},
 title = {Interactions with big data analytics},
 type = {},
 video = {http://research.microsoft.com/apps/pubs/default.aspx?id=217732},
 volume = {19},
 year = {2012}
}

@inproceedings{fisher2012trust,
 author = {Fisher, Danyel and Popov, Igor and Drucker, Steven and others},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 caption = {Incremental Visualization},
 editor = {},
 facet.collaborators = {Fisher,Popov,schraefel},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Information,Visualization,Big Data},
 facet.year = {2012},
 id = {64.0},
 img = {researchImages/incvis.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Queries over large scale (petabyte) data bases often mean waiting overnight for a result to come back. Scale costs  time. Such time also means that potential avenues of  exploration are ignored because the costs are perceived to  be too high to run or even propose them. With  sampleAction we have explored whether interaction  techniques to present query results running over only  incremental samples can be presented as sufficiently  trustworthy for analysts both to make closer to real time  decisions about their queries and to be more exploratory in  their questions of the data. Our work with three teams of  analysts suggests that we can indeed accelerate and open up  the query process with such incremental visualizations.},
 pages = {1673--1682},
 pdf = {http://research.microsoft.com/pubs/163220/chi2012_interactive.pdf},
 primary = {Visualization},
 publisher = {},
 reference = {Danyel Fisher, Igor Popov, Steven M. Drucker, and mc schraefel, Trust Me, I'm Partially Right: Incremental Visualization Lets Analysts Explore Large Datasets Faster, in Proceedings of the 2012 Conference on Human Factors in Computing Systems (CHI 2012), ACM Conference on Human Factors in Computing Systems, 5 May 2012},
 school = {},
 text = {chi2012_interactive.txt},
 thumb = {thumbnail/incvis.png},
 title = {Trust me, I'm partially right: incremental visualization lets analysts explore large datasets faster},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/timeslicehd.m4v},
 volume = {},
 year = {2012}
}

@article{fisher2014business,
 author = {Fisher, Danyel and Drucker, Steven and Czerwinski, Mary},
 booktitle = {},
 caption = {Bi Analytics},
 editor = {},
 facet.collaborators = {Fisher,Drucker},
 facet.publication = {CGA},
 facet.subject = {Information,Visualization},
 facet.year = {2015},
 id = {80.0},
 img = {researchImages/BI.png},
 institution = {},
 journal = {IEEE computer graphics and applications},
 link = {},
 month = {},
 number = {5},
 organization = {},
 abstract = {Businesses are increasingly monitoring and tracking data about what it takes to keep themselves running. They collect and maintain increasingly available data, such as ? transaction and sales data stored in data warehouses, ? server log files tracking visitors, ? data from sensors tracking delays on factory floors, ? IT data logs, and ? data on their competitors and industrial sectors. Data-driven decision making?orienting business decisions around data?drives major IT initiatives across all business sectors},
 pages = {22--24},
 pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/mcg2014050022.pdf},
 primary = {UI-Information},
 publisher = {IEEE},
 reference = {Fisher, Drucker. Editor's Introduction to Special Issue on Business Intelligence, IEEE CG&A, 2015},
 school = {},
 text = {BIEditorIntro.txt},
 thumb = {thumbnail/bi.png},
 title = {Business Intelligence Analytics [Guest editors' introduction]},
 type = {},
 video = {},
 volume = {34},
 year = {2014}
}

@techreport{flatland,
 author = {Harry Chesley, Steven M. Drucker, Anoop Gupta, Greg Kimberly, Steven White},
 booktitle = {},
 caption = {Flatland},
 editor = {},
 facet.collaborators = {Chesley,Gupta,Kimberly,White},
 facet.publication = {InternalReport},
 facet.subject = {UI,Education,Social},
 facet.year = {2001},
 id = {23.0},
 img = {researchImages/flatland.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {MSR-TR-2001-73},
 organization = {},
 abstract = {Computer intra- and internets are widely used for client-server application such as web browsers. With the exception of e-mail, however, the same networks are seldom used for distributed, client-client or client-server-client applications. Such applications are difficult to develop and debug, and require a supporting infrastructure that is not readily available from existing systems. Flatland is a rapid prototyping environment that provides the underlying infrastructure and makes it easy to create and debug distributed internet application prototypes. In addition to the infrastructure needed for a distributed application, Flatland includes safe implementations of the most common sources of distributed application bugs, asynchronous operation and updating. Flatland also supports streaming audio-video and down-level clients.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/flatland.pdf},
 primary = {Social},
 publisher = {},
 reference = {Chesley, H. Drucker, S. Gupta, A., Kimberly, G. White, S.  Flatland: Rapid prototyping of distributed internet applications.MSR-TR-2001-73.},
 school = {},
 text = {flatland.txt},
 thumb = {thumbnail/flatland.jpg},
 title = {Flatland, Rapid prototyping of distributed internet applications},
 type = {},
 video = {},
 volume = {},
 year = {2001}
}

@inproceedings{gemmell2002mylifebits,
 author = {Gemmell, Jim and Bell, Gordon and Lueder, Roger and Drucker, Steven and Wong, Curtis},
 booktitle = {Proceedings of the tenth ACM international conference on Multimedia},
 caption = {Memex Vision},
 editor = {},
 facet.collaborators = {Gemmell,Bell,Lueder,Wong},
 facet.publication = {Multimedia},
 facet.subject = {UI,Media,Search,Information},
 facet.year = {2002},
 id = {25.0},
 img = {researchImages/memex1.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {MyLifeBits is a project to fulfill the Memex vision first posited by Vannevar Bush in 1945. It is a system for storing all of one???s digital media, including documents, images, sounds, and videos. It is built on four principles: (1) collections and search must replace hierarchy for organization (2) many visualizations should be supported (3) annotations are critical to non-text media and must be made easy, and (4) authoring should be via transclusion.},
 pages = {235--238},
 pdf = {},
 primary = {UI-Information},
 publisher = {},
 reference = {Gemmell, J., Bell, G., Lueder, R., Drucker, S., & Wong, C. (2002, December). MyLifeBits: fulfilling the Memex vision. In Proceedings of the tenth ACM international conference on Multimedia (pp. 235-238). ACM.},
 school = {},
 text = {},
 thumb = {thumbnail/memex1.png},
 title = {MyLifeBits: fulfilling the Memex vision},
 type = {},
 video = {},
 volume = {},
 year = {2002}
}

@article{guenter2012foveated,
 author = {Guenter, Brian and Finch, Mark and Drucker, Steven and Tan, Desney and Snyder, John},
 booktitle = {},
 caption = {Foveated},
 editor = {},
 facet.collaborators = {Guenter,Finch,Tan,Snyder},
 facet.publication = {SIGGRAPHAsia},
 facet.subject = {Graphics,Photos},
 facet.year = {2012},
 id = {63.0},
 img = {researchImages/foveated.png},
 institution = {},
 journal = {ACM Transactions on Graphics (TOG)},
 link = {},
 month = {},
 number = {6},
 organization = {},
 abstract = {We present a data-driven method to predict the performance of an image completion method. Our image completion method is based on the state-of-the-art non-parametric framework of Wexler et al. [2007]. It uses automatically derived search space constraints for patch source regions, which lead to improved texture synthesis and semantically more plausible results. These constraints also facilitate performance prediction by allowing us to correlate output quality against features of possible regions used for synthesis. We use our algorithm to first crop and then complete stitched panoramas. Our predictive ability is used to find an optimal crop shape before the completion is computed, potentially saving significant amounts of computation. Our optimized crop includes as much of the original panorama as possible while avoiding regions that can be less successfully filled in. Our predictor can also be applied for hole filling in the interior of images. In addition to extensive comparative results, we ran several user studies validating our predictive feature, good relative quality of our results against those of other state-of-the-art algorithms, and our automatic cropping algorithm.},
 pages = {164},
 pdf = {http://research.microsoft.com/apps/pubs/default.aspx?id=176610},
 primary = {Graphics},
 publisher = {ACM},
 reference = {Foveated 3D Graphics, Brian Guenter, Mark Finch, Steven Drucker, Desney Tan, John Snyder ACM SIGGRAPH Asia 2012.},
 school = {},
 text = {foveateduserstudy07.txt},
 thumb = {thumbnail/foveated.png},
 title = {Foveated 3D graphics},
 type = {},
 video = {},
 volume = {31},
 year = {2012}
}

@inproceedings{hua2007face,
 author = {Hua, Gang and Viola, Paul A and Drucker, Steven M},
 booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
 caption = {FaceRec},
 editor = {},
 facet.collaborators = {Hua,Viola},
 facet.publication = {CVPR},
 facet.subject = {Machine Learning},
 facet.year = {2007},
 id = {46.0},
 img = {researchImages/facerec.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {IEEE},
 abstract = {We propose a method for face recognition based on a discriminative linear projection. In this formulation images are treated as tensors, rather than the more conventional vector of pixels. Projections are pursued sequentially and take the form of a rank one tensor, i.e., a tensor which is the outer product of a set of vectors. A novel and effective technique is proposed to ensure that the rank one tensor projections are orthogonal to one another. These constraints on the tensor projections provide a strong inductive bias and result in better generalization on small training sets. Our work is related to spectrum methods, which achieve orthogonal rank one projections by pursuing consecutive projections in the complement space of previous projections. Although this may be meaningful for applications such as reconstruction, it is less meaningful for pursuing discriminant projections. Our new scheme iteratively solves an eigenvalue problem with orthogonality constraints on one dimension, and solves unconstrained eigenvalue problems on the other dimensions. Experiments demonstrate that on small and medium sized face recognition datasets, this approach outperforms previous embedding methods. On large face datasets this approach achieves results comparable with the best, often using fewer discriminant projections.},
 pages = {1--8},
 pdf = {http://users.eecs.northwestern.edu/~ganghua/publication/CVPR07a.pdf},
 primary = {Machine Learning},
 publisher = {},
 reference = {Gang Hua, Paul Viola, and Steven Drucker, "Face Recognition using Discriminatively Trained Orthogonal Rank One Tensor Projections", in Proc.?IEEE Conf. on Computer Vision and Pattern Recognition (CVPR'2007),?Minneaplois, MN, 2007.},
 school = {},
 text = {CVPR07a.txt},
 thumb = {thumbnail/facerec.png},
 title = {Face recognition using discriminatively trained orthogonal rank one tensor projections},
 type = {},
 video = {},
 volume = {},
 year = {2007}
}

@article{hullman2013deeper,
 author = {Hullman, Jessica and Drucker, Steven and Riche, Nathalie Henry and Lee, Bongshin and Fisher, Danyel and Adar, Eytan},
 booktitle = {},
 caption = {Narrative},
 editor = {},
 facet.collaborators = {Hullman,Riche,Fisher,Adar,Lee},
 facet.publication = {Infovis},
 facet.subject = {Information,Visualization,Presentation},
 facet.year = {2013},
 id = {71.0},
 img = {researchImages/narratives.png},
 institution = {},
 journal = {IEEE transactions on visualization and computer graphics},
 link = {},
 month = {},
 number = {12},
 organization = {},
 abstract = {Conveying a narrative with visualizations often requires choosing an order in which to present visualizations. While evidence exists that narrative sequencing in traditional stories can affect comprehension and memory, little is known about how sequencing choices affect narrative visualization. We consider the forms and reactions to sequencing in narrative visualization presentations to provide a deeper understanding with a focus on linear, slideshow-style presentations. We conduct a qualitative analysis of 42 professional narrative visualizations to gain empirical knowledge on the forms that structure and sequence take. Based on the results of this study we propose a graph-driven approach for automatically identifying effective sequences in a set of visualizations to be presented linearly. Our approach identifies possible transitions in a visualization set and prioritizes local (visualization-to-visualization) transitions based on an objective function that minimizes the cost of transitions from the audience perspective. We conduct two studies to validate this function. We also expand the approach with additional knowledge of user preferences for different types of local transitions and the effects of global sequencing strategies on memory, preference, and comprehension. Our results include a relative ranking of types of visualization transitions by the audience perspective and support or memory and subjective rating benefits of visualization sequences that use parallelism as a structural device. We discuss how these insights can guide the design of narrative visualization and systems that support optimization of visualization sequence.},
 pages = {2406--2415},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/narrative.pdf},
 primary = {Visualization},
 publisher = {IEEE},
 reference = {Hullman, J., Drucker, S., Henry Riche, N., Lee, B., Fisher, D., & Adar, E. (2013). A deeper understanding of sequence in narrative visualization. Visualization and Computer Graphics, IEEE Transactions on, 19(12), 2406-2415.},
 school = {},
 text = {story_sequence_infovis_final.txt},
 thumb = {thumbnail/narratives.png},
 title = {A deeper understanding of sequence in narrative visualization},
 type = {},
 video = {http://research.microsoft.com/apps/video/default.aspx?id=188294},
 volume = {19},
 year = {2013}
}

@inproceedings{huynh2005time,
 author = {Huynh, David F and Drucker, Steven M and Baudisch, Patrick and Wong, Curtis},
 booktitle = {CHI'05 extended abstracts on Human factors in computing systems},
 caption = {TimeQuilt},
 editor = {},
 facet.collaborators = {Huynh,Baudisch,Wong},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Photos,Visualization},
 facet.year = {2005},
 id = {35.0},
 img = {researchImages/tq.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {In the absence of manual organization of large digital photo collections, the photos' visual content and creation dates can help support time-based visual search tasks. Current zoomable photo browsers are designed to support visual searches by maximizing screenspace usage. However, their space-filling layouts fail to convey temporal order effectively. We propose a novel layout called time quilt that trades off screenspace usage for better presentation of temporal order. In an experimental comparison of space-filling, linear timeline, and time quilt layouts, participants carried out the task of finding photos in their personal photo collections averaging 4,000 items. They performed 45% faster on time quilt. Furthermore, while current zoomable photo browsers are designed for visual searches, this support does not scale to thousands of photos: individual thumbnails become less informative as they grow smaller. We found a subjective preference for the use of representative photos to provide an overview for visual searches in place of the diminishing thumbnails.},
 pages = {1937--1940},
 pdf = {http://research.microsoft.com/~sdrucker/papers/CHI2005%20-%20Time%20Quilt%20short.pdf},
 primary = {Photos},
 publisher = {},
 reference = {Huynh, D., Drucker, S., Baudisch, P., Wong, C. Time Quilt: Scaling up Zoomable Photo Browsers for Large, Unstructured Photo Collections. CHI 2005. Portland, OR. Apr. 2005},
 school = {},
 text = {Time Quilt Long.txt},
 thumb = {thumbnail/tq.png},
 title = {Time quilt: scaling up zoomable photo browsers for large, unstructured photo collections},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/timequilt.wmv},
 volume = {},
 year = {2005}
}

@techreport{imageflow-streaming-image-search,
 author = {Varun Jampani, Gonzalo Ramos, Steven Drucker},
 booktitle = {},
 caption = {Imageflow},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {Microsoft Research},
 journal = {},
 link = {https://www.microsoft.com/en-us/research/publication/imageflow-streaming-image-search/},
 month = {November},
 number = {},
 organization = {},
 abstract = {Traditional grid and list representations of image search results are the dominant interaction paradigms that users face on a daily basis, yet it is unclear that such paradigms are well-suited for experiences where the user's task is to browse images for leisure, to discover new information or to seek particular images to represent ideas. We introduce ImageFlow, a novel image search user interface that explores a different alternative to the traditional presentation of image search results. ImageFlow presents image results on a canvas where we map semantic features (e.g., relevance, related queries) to the canvas' spatial dimensions (e.g., x, y, z) in a way that allows for several levels of engagement – from passively viewing a stream of images, to seamlessly navigating through the semantic space and actively collecting images for sharing and reuse. We have implemented our system as a fully functioning prototype, and we report on promising, preliminary usage results.},,
 pages = {},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {ImageFlow: Streaming Image Search},
 type = {},
 video = {},
 volume = {},
 year = {2010}
}

@inproceedings{jensen2000effect,
 author = {Jensen, Carlos and Farnham, Shelly D and Drucker, Steven M and Kollock, Peter},
 booktitle = {Proceedings of the SIGCHI conference on Human Factors in Computing Systems},
 caption = {Social Dilemma Testing},
 editor = {},
 facet.collaborators = {Jensen,Farnham,Kollock},
 facet.publication = {SIGCHI},
 facet.subject = {Social},
 facet.year = {2000},
 id = {18.0},
 img = {researchImages/socdilemma.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {One of the most robust findings in the sociological literature is the positive effect of communication on cooperation and trust.  When individuals are able to communicate, cooperation increases significantly.  How does the choice of communication modality influence this effect?  We adapt the social dilemma research paradigm to quantitatively analyze different modes of communication. Using this method, we compare four forms of communication: no communication, text-chat, text-to-speech, and voice.  We found statistically significant differences between the various forms of communication, with the voice condition resulting in the highest levels of cooperation.  Our results highlight the importance of striving towards the use of more advanced forms of communication in online environments, especially where trust and cooperation are essential.  In addition, our research demonstrates the applicability of the social dilemma paradigm in testing the extent to which communication modalities promote the development of trust and cooperation. },
 pages = {470--477},
 pdf = {http://research.microsoft.com/~sdrucker/papers/chidilemmas.pdf},
 primary = {Social},
 publisher = {},
 reference = {Jensen, C., Farnham, S., Drucker, S., & Kollock, P. The Effect of Communication Modality on Cooperation in Online Environments. In Proceedings of CHI 2000, The Hague, Netherlands March 2000.},
 school = {},
 text = {chidilemmas.txt},
 thumb = {thumbnail/socdilemma.gif},
 title = {The effect of communication modality on cooperation in online environments},
 type = {},
 video = {},
 volume = {},
 year = {2000}
}

@inproceedings{joshi2012cliplets,
 author = {Joshi, Neel and Mehta, Sisil and Drucker, Steven and Stollnitz, Eric and Hoppe, Hugues and Uyttendaele, Matt and Cohen, Michael},
 booktitle = {Proceedings of the 25th annual ACM symposium on User interface software and technology},
 caption = {Cliplets},
 editor = {},
 facet.collaborators = {Joshi,Metha,Stollnitz,Cohen,Hoppe,Uyttendaele},
 facet.publication = {UIST},
 facet.subject = {UI,Graphics,Photos},
 facet.year = {2012},
 id = {61.0},
 img = {researchImages/cliplets.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {We explore creating cliplets, a form of visual media that juxtaposes still image and video segments, both spatially and temporally, to expressively abstract a moment. Much as in cinemagraphs, the tension between static and dynamic elements in a cliplet reinforces both aspects, strongly focusing the viewer's attention. Creating this type of imagery is challenging without professional tools and training. We develop a set of idioms, essentially spatiotemporal mappings, that characterize cliplet elements, and use these idioms in an interactive system to quickly compose a cliplet from ordinary handheld video. One difficulty is to avoid artifacts in the cliplet composition without resorting to extensive manual input. We address this with automatic alignment, looping optimization and feathering, simultaneous matting and compositing, and Laplacian blending. A key user-interface challenge is to provide affordances to define the parameters of the mappings from input time to output time while maintaining a focus on the cliplet being created. We demonstrate the creation of a variety of cliplet types. We also report on informal feedback as well as a more structured survey of users.},
 pages = {251--260},
 pdf = {https://research.microsoft.com/en-us/um/redmond/projects/ClipletsDesktop/paper/paper_uist_final.pdf},
 primary = {Photos},
 publisher = {},
 reference = {Joshi, N., Metha, S., Drucker, S., Stollnitz, E., Hoppe, H., Uyttendaele, M., and Cohen, M. Cliplets: Juxtaposing Still and Dynamic Imagery. ACM UIST 2012.},
 school = {},
 text = {cliplets.txt},
 thumb = {thumbnail/cliplets.png},
 title = {Cliplets: juxtaposing still and dynamic imagery},
 type = {},
 video = {},
 volume = {},
 year = {2012}
}

@inproceedings{kairam2015refinery,
 author = {Kairam, Sanjay and Riche, Nathalie Henry and Drucker, Steven and Fernandez, Roland and Heer, Jeffrey},
 booktitle = {Computer Graphics Forum},
 caption = {Refinery},
 editor = {},
 facet.collaborators = {Kairam,Riche,Fernandez,Heer},
 facet.publication = {EuroVis},
 facet.subject = {Information,Visualization,Network},
 facet.year = {2015},
 id = {77.0},
 img = {researchImages/refinery.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {3},
 organization = {},
 abstract = {Browsing is a fundamental aspect of exploratory information-seeking. Associative browsing encompasses a common and intuitive set of exploratory strategies in which users step iteratively from familiar to novel pieces of information. In this paper, we consider associative browsing as a strategy for bottom-up exploration of large, heterogeneous networks. We present Refinery, an interactive visualization system informed by guidelines drawn from examination of several areas of literature related to exploratory information-seeking. These guidelines motivate Refinery???s query model, which allows users to simply and expressively construct queries using heterogeneous sets of nodes. The system ranks and returns associated content using a fast, random-walk based algorithm, visualizing results and connections among them to provide explanatory context, facilitate serendipitous discovery, and stimulate continued exploration. A study of 12 academic researchers using Refinery to browse publication data related to areas of study demonstrates how the system complements existing tools in supporting discovery. },
 pages = {301--310},
 pdf = {},
 primary = {Visualization},
 publisher = {},
 reference = {Kairam, S., N.H. Riche, S. Drucker, R. Fernandeaz, J. Heer, Refinery: Visual Exploration of Large, Heterogeneous Networks through Associative Browsing, To Appear Eurographics Conference on Visualization, (EuroVis) 2015.},
 school = {},
 text = {refinery-eurovis15-fin.txt},
 thumb = {thumbnail/refinery.png},
 title = {Refinery: Visual exploration of large, heterogeneous networks through associative browsing},
 type = {},
 video = {},
 volume = {34},
 year = {2015}
}

@inproceedings{kerne2013evaluation,
 author = {Kerne, Andruid and Webb, Andrew M and Latulipe, Celine and Carroll, Erin and Drucker, Steven M and Candy, Linda and H{\"o}{\"o}k, Kristina},
 booktitle = {CHI'13 Extended Abstracts on Human Factors in Computing Systems},
 caption = {},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {},
 pages = {3295--3298},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Evaluation methods for creativity support environments},
 type = {},
 video = {},
 volume = {},
 year = {2013}
}

@article{kopf2012quality,
 author = {Kopf, Johannes and Kienzle, Wolf and Drucker, Steven and Kang, Sing Bing},
 booktitle = {},
 caption = {Completion},
 editor = {},
 facet.collaborators = {Kopf,Kienzle,Kang},
 facet.publication = {SIGGRAPHAsia},
 facet.subject = {Graphics,Photos},
 facet.year = {2012},
 id = {62.0},
 img = {researchImages/completion.png},
 institution = {},
 journal = {ACM Transactions on Graphics (TOG)},
 link = {},
 month = {},
 number = {6},
 organization = {},
 abstract = {We present a data-driven method to predict the performance of an image completion method. Our image completion method is based on the state-of-the-art non-parametric framework of Wexler et al. [2007]. It uses automatically derived search space constraints for patch source regions, which lead to improved texture synthesis and semantically more plausible results. These constraints also facilitate performance prediction by allowing us to correlate output quality against features of possible regions used for synthesis. We use our algorithm to first crop and then complete stitched panoramas. Our predictive ability is used to find an optimal crop shape before the completion is computed, potentially saving significant amounts of computation. Our optimized crop includes as much of the original panorama as possible while avoiding regions that can be less successfully filled in. Our predictor can also be applied for hole filling in the interior of images. In addition to extensive comparative results, we ran several user studies validating our predictive feature, good relative quality of our results against those of other state-of-the-art algorithms, and our automatic cropping algorithm.},
 pages = {131},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/completion.pdf},
 primary = {Photos},
 publisher = {ACM},
 reference = {Johannes Kopf, Wolf Kienzle, Steven Drucker, Sing Bing Kang, Quality Prediction for Image Completion, ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2012), 31(6), Article no. 196, 2012},
 school = {},
 text = {completion.txt},
 thumb = {thumbnail/completion.png},
 title = {Quality prediction for image completion},
 type = {},
 video = {http://research.microsoft.com/apps/video/default.aspx?id=173013},
 volume = {31},
 year = {2012}
}

@article{liu2015exploring,
 author = {Liu, Shixia and Chen, Yang and Wei, Hao and Yang, Jing and Zhou, Kun and Drucker, Steven M},
 booktitle = {},
 caption = {Topic-Lead-Lag},
 editor = {},
 facet.collaborators = {Liu,Chen,Wei,Yang,Zhou},
 facet.publication = {TKDE},
 facet.subject = {Information,Visualization,Machine Learning},
 facet.year = {2014},
 id = {75.0},
 img = {researchImages/leadlag.png},
 institution = {},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 link = {},
 month = {},
 number = {1},
 organization = {},
 abstract = {Identifying which text corpus leads in the context of a topic presents a great challenge of considerable interest to researchers. Recent research into lead-lag analysis has mainly focused on estimating the overall leads and lags between two corpora. However, real-world applications have a dire need to understand lead-lag patterns both globally and locally. In this paper, we introduce TextPioneer, an interactive visual analytics tool for investigating lead-lag across corpora from the global level to the local level. In particular, we extend an existing lead-lag analysis approach to derive two-level results. To convey multiple perspectives of the results, we have designed two visualizations, a novel hybrid tree visualization that couples a radial space-filling tree with a node-link diagram and a twisted-ladder-like visualization. We have applied our method to several corpora and the evaluation shows promise, especially in support of text comparison at different levels of detail.},
 pages = {115--129},
 pdf = {http://research.microsoft.com/en-us/um/people/shliu/TextPioneerTKDE.pdf},
 primary = {Visualization},
 publisher = {IEEE},
 reference = {Liu, S., Chen, Y., Wei, H., Yang, J., Zhou, K., & Drucker, S. M. Exploring Topical Lead-Lag across Corpora. IEEE TKDE},
 school = {},
 text = {textpioneerpaper.txt},
 thumb = {thumbnail/leadlag.png},
 title = {Exploring topical lead-lag across corpora},
 type = {},
 video = {http://research.microsoft.com/en-us/people/sdrucker/video/demowiz.mp4},
 volume = {27},
 year = {2015}
}

@misc{llthumbtack,
 author = {Steven M. Drucker, Aamer Hydrie},
 booktitle = {},
 caption = {LiveLabs Thumbtack},
 editor = {},
 facet.collaborators = {Hydrie,Cutler,Oliveira,Bergeron,Lakshmiratan},
 facet.publication = {InternalReport},
 facet.subject = {UI,Information,Web},
 facet.year = {2009},
 id = {54.0},
 img = {researchImages/thumbtackthumb.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Thumbtack is an easy way to save links, photos, and anything else you can find on bunch of different Web sites to a single place.  Grab the stuff you want, put it into a Thumbtack collection, then get to it from anywhere you can get online.  Share it with your friends, or just keep it for yourself. It's way easier than sending a bunch of links in an e-mail, and even easier than setting lots of favorites in your browser.},
 pages = {},
 pdf = {},
 primary = {UI-Information},
 publisher = {},
 reference = {Internal Report},
 school = {},
 text = {},
 thumb = {thumbnail/thumbtackthumb.jpg},
 title = {LiveLabs ThumbTack},
 type = {software},
 video = {},
 volume = {},
 year = {2009}
}

@inproceedings{luan2008annotating,
 author = {Luan, Qing and Drucker, Steven M and Kopf, Johannes and Xu, Ying-Qing and Cohen, Michael F},
 booktitle = {Proceedings of the 21st annual ACM symposium on User interface software and technology},
 caption = {Annotation Gigapixel Images},
 editor = {},
 facet.collaborators = {Cohen,Luan,Kopf,Xu},
 facet.publication = {UIST},
 facet.subject = {Visualization,UI,Photos},
 facet.year = {2008},
 id = {48.0},
 img = {researchImages/annotategigapixel.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Panning and zooming interfaces for exploring very large images containing billions of pixels (gigapixel images) have recently appeared on the internet. This paper addresses issues that arise when creating and rendering auditory and textual annotations for such images. In particular, we define a distance metric between each annotation and any view resulting from panning and zooming on the image. The distance then informs the rendering of audio annotations and text labels. We demonstrate the annotation system on a number of panoramic images.},
 pages = {33--36},
 pdf = {http://research.microsoft.com/~sdrucker/papers/uist2008annotating.pdf},
 primary = {Photos},
 publisher = {},
 reference = {Luan, Q, Drucker, S.M., Kopf, J., Xu, Y, Cohen, M.F. Annotating Gigapixel Images, UIST 2008},
 school = {},
 text = {uist2008annotating.txt},
 thumb = {thumbnail/annotategigapixel.jpg},
 title = {Annotating gigapixel images},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/Pixaura_CHI_08_v3.mov},
 volume = {},
 year = {2008}
}

@inproceedings{medynskiy2009exploring,
 author = {Medynskiy, Yevgeniy and Dontcheva, Mira and Drucker, Steven M},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 caption = {Contextual Facets},
 editor = {},
 facet.collaborators = {Dontcheva,Medynskiy},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Information,Search},
 facet.year = {2009},
 id = {51.0},
 img = {researchImages/thumbnail_contextualfacets.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {We present contextual facets, a novel user interface technique for navigating websites that publish large collections of semi-structured data. Contextual facets extend traditional faceted navigation techniques by transforming webpage elements into user interface components for filtering and retrieving related webpages. To investigate users' reactions to contextual facets, we built FacetPatch, a web browser that automatically generates contextual facet interfaces. As the user browses the web, FacetPatch automatically extracts semi-structured data from collections of webpages and overlays contextual facets on top of the current page. Participants in an exploratory user evaluation of FacetPatch were enthusiastic about contextual facets and often preferred them to an existing, familiar faceted navigation interface. We discuss how we improved the design of contextual facets and FacetPatch based on the results of this study.},
 pages = {2013--2022},
 pdf = {http://research.microsoft.com/~sdrucker/papers/2008CHI_Contextual_Facets.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Medynskiy, Y., Dontcheva, M. Drucker, S.M., Exploring Websites through Contextual Facets, SIGCHI 2009.},
 school = {},
 text = {2008CHI_Contextual_Facets.txt},
 thumb = {thumbnail/thumbnail_contextualfacets.png},
 title = {Exploring websites through contextual facets},
 type = {},
 video = {},
 volume = {},
 year = {2009}
}

@inproceedings{meyers2006dance,
 author = {Meyers, Brian and Brush, AJ and Drucker, Steven and Smith, Marc A and Czerwinski, Mary},
 booktitle = {CHI'06 extended abstracts on Human factors in computing systems},
 caption = {Step User Interfaces},
 editor = {},
 facet.collaborators = {Meyers,Brush,Smith,Czerwinski},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Photos},
 facet.year = {2006},
 id = {43.0},
 img = {researchImages/stepUI27.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {While applications are typically optimized for traditional desktop interfaces using a keyboard and mouse, there are a variety of compelling reasons to consider alternative input mechanisms that require more physical exertion, including promoting fitness, preventing Repetitive Strain Injuries, and encouraging fun. We chose to explore physical interfaces based on foot motion and have built two applications with Step User Interfaces: StepMail and StepPhoto. Both support working with email and photos using the dance pad made popular by the Dance Dance Revolution (DDR) game. Results of a formative evaluation with ten participants suggest that the interactions are intuitive to learn, somewhat enjoyable, and cause participants to increase their level of exertion over sitting at a desk. Our evaluation also revealed design considerations for Step User Interfaces, including balancing effort across the body, avoiding needless exertion, and choosing target applications with care.},
 pages = {387--392},
 pdf = {http://research.microsoft.com/~sdrucker/papers/stepUICHI06.pdf},
 primary = {Photos},
 publisher = {},
 reference = {Meyers, B., Brush, A. B., Drucker, S., Smith, M. A., and Czerwinski, M. 2006. Dance your work away: exploring step user interfaces. In CHI '06 Human Factors in Computing Systems (Montr??al, Qu??bec, Canada, April 22 - 27, 2006). CHI '06. ACM Press, New York, NY, 387-392.},
 school = {},
 text = {stepUICHI06.txt},
 thumb = {thumbnail/stepUI27.jpg},
 title = {Dance your work away: exploring step user interfaces},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/StepStar.wmv},
 volume = {},
 year = {2006}
}

@article{morris2014reducing,
 author = {Morris, Meredith Ringel and Danielescu, Andreea and Drucker, Steven and Fisher, Danyel and Lee, Bongshin and Wobbrock, Jacob O and others},
 booktitle = {},
 caption = {GestureElicitation},
 editor = {},
 facet.collaborators = {Morris,Danielescu,Fisher,Lee,schraefel,Wobbrock},
 facet.publication = {Interact},
 facet.subject = {UI},
 facet.year = {2013},
 id = {68.0},
 img = {researchImages/gestureelicitation.png},
 institution = {},
 journal = {interactions},
 link = {},
 month = {},
 number = {3},
 organization = {},
 abstract = {Improving methods for choosing appropriate gestures for novel user interaction techniques.},
 pages = {40--45},
 pdf = {http://research.microsoft.com/apps/pubs/default.aspx?id=208568},
 primary = {UI-Information},
 publisher = {ACM},
 reference = {Meredith Ringel Morris, Andreea Danielescu, Steven Drucker, Danyel Fisher, Bongshin Lee, m.c. schraefel, and Jacob O. Wobbrock, Reducing Legacy Bias in Gesture Elicitation Studies, in ACM Interactions Magazine, ACM, May 2014},
 school = {},
 text = {gesture_elicitation_interactions.txt},
 thumb = {thumbnail/gestureelicitation.png},
 title = {Reducing legacy bias in gesture elicitation studies},
 type = {},
 video = {http://research.microsoft.com/apps/pubs/default.aspx?id=208568},
 volume = {21},
 year = {2014}
}

@misc{movievariations,
 author = {Steven M. Drucker, Curtis Wong},
 booktitle = {},
 caption = {Movie Variations},
 editor = {},
 facet.collaborators = {Roseway,De Mar,Wong},
 facet.publication = {InternalReport},
 facet.subject = {UI,Information,Visualization,Movies},
 facet.year = {2003},
 id = {29.0},
 img = {researchImages/MV_photo.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {This system allows for browsing a movie collection by moving from one related group of movies to another related group, where groups are related by common actor or director. As the user selects a movie from the cluster, it moves to the center and 4 related clusters are moved arranged around the movie. Extensions can include clusters that are related by collaborative filtering or other common features.},
 pages = {},
 pdf = {},
 primary = {Media},
 publisher = {},
 reference = {Drucker, S. Movie Variations, MS Tech Report},
 school = {},
 text = {},
 thumb = {thumbnail/MV_photo.jpg},
 title = {Movie Variations},
 type = {software},
 video = {http://research.microsoft.com/~sdrucker/Video/mbrowse2.wmv},
 volume = {},
 year = {2003}
}

@inproceedings{patel2010gestalt,
 author = {Patel, Kayur and Bancroft, Naomi and Drucker, Steven M and Fogarty, James and Ko, Andrew J and Landay, James},
 booktitle = {Proceedings of the 23nd annual ACM symposium on User interface software and technology},
 caption = {Gestalt},
 editor = {},
 facet.collaborators = {Patel,Bancroft,Fogarty,Ko,Landay},
 facet.publication = {UIST},
 facet.subject = {UI,Information,Visualization,Machine Learning,Programming},
 facet.year = {2010},
 id = {57.0},
 img = {researchImages/gestalt.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {We present Gestalt, a development environment designed tosupport the process of applying machine learning. While traditional programming environments focus on source code, we explicitly support both code and data. Gestalt allows developers to implement a classification pipeline, analyze data as it moves through that pipeline, and easily transition between implementation and analysis. An experiment shows this significantly improves the ability of developers to find and fix bugs in machine learning systems. Our discussion of Gestalt and our experimental observations provide new insight into general-purpose support for the  achine learning process.},
 pages = {37--46},
 pdf = {http://research.microsoft.com/~sdrucker/papers/uist2010gestalt.pdf},
 primary = {Machine Learning},
 publisher = {},
 reference = {Patel, K, N. Bancroft, S.M.Drucker, J. Fogarty, A. Ko, J.Landay, Gestalt: Integrated Support for Implementation and Analysis in Machine Learning},
 school = {},
 text = {uist2010gestalt.txt},
 thumb = {thumbnail/gestalt.png},
 title = {Gestalt: integrated support for implementation and analysis in machine learning},
 type = {},
 video = {http://www.youtube.com/watch?v=9XC-D2L93jA&feature=player_embedded},
 volume = {},
 year = {2010}
}

@inproceedings{patel2011using,
 author = {Patel, Kayur and Drucker, Steven M and Fogarty, James and Kapoor, Ashish and Tan, Desney S},
 booktitle = {IJCAI Proceedings-International Joint Conference on Artificial Intelligence},
 caption = {Prospect},
 editor = {},
 facet.collaborators = {Patel,Kapoor,Fogarty,Ko,Tan},
 facet.publication = {IJCAI},
 facet.subject = {UI,Visualization,Machine Learning},
 facet.year = {2011},
 id = {58.0},
 img = {researchImages/prospect.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {1},
 organization = {},
 abstract = {A human's ability to diagnose errors, gather data, and generate features in order to build better models is largely untapped. We hypothesize that analyzing results from multiple models can help people diagnose errors by understanding relationships among data, features, and algorithms. These relationships might otherwise be masked by the bias inherent to any individual model. We demonstrate this approach in our Prospect system, show how multiple models can be used to detect label noise and aid in generating new features, and validate our methods in a pair of experiments. },
 pages = {1723},
 pdf = {http://research.microsoft.com/~sdrucker/papers/ijcai11.pdf},
 primary = {Machine Learning},
 publisher = {},
 reference = {Patel, K, S.M.Drucker, J. Fogarty, A. Kapoor, D.S.Tan, Prospect: Using Multiple Models to Understand Data, Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2011)},
 school = {},
 text = {ijcai11.txt},
 thumb = {thumbnail/prospect.png},
 title = {Using multiple models to understand data},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/visavis.wmv},
 volume = {22},
 year = {2011}
}

@misc{peoplebrowser,
 author = {Steven M. Drucker, Curtis Wong},
 booktitle = {},
 caption = {People Browser},
 editor = {},
 facet.collaborators = {Wong},
 facet.publication = {InternalReport},
 facet.subject = {UI,Visualization,Information},
 facet.year = {2003},
 id = {30.0},
 img = {researchImages/peoplebrowser_photo.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {The concept of 6 degrees separation (6 DOS) can be applied to many different domains. As demonstrated in the MediaVariations Browser, movies can easily be browsed using clusters of related movies using the actor and director to help associate movies. Looking at people is even more natural for this type of browsing, since that is what the concept of 6 DOS is usually associated with. The PeopleBrowser uses a person's rank within an organization, their management chain, their peers (under the same manager), their direct reports, and people with their same title, to help browse through an organization. Other clusters could also easily be used, including those people on the same mailing list, frequently mailed, etc. This project was done in conjunction with the Shell MSX team (Hillel Cooperman, Rob Girling, and Jeni Sadler).},
 pages = {},
 pdf = {},
 primary = {UI-Information},
 publisher = {},
 reference = {Drucker, S, People Browser, MS Internal Report},
 school = {},
 text = {},
 thumb = {thumbnail/peoplebrowser_photo.jpg},
 title = {People Browser},
 type = {software},
 video = {http://research.microsoft.com/~sdrucker/Video/peoplebrowser.avi},
 volume = {},
 year = {2003}
}

@misc{pivotviewer,
 author = {Gary Flake, Karim Farouki, Brett Brewer, Steven M. Drucker},
 booktitle = {},
 caption = {LiveLabs Pivot Viewer},
 editor = {},
 facet.collaborators = {Flake,Brewer},
 facet.publication = {InternalReport},
 facet.subject = {UI,Information,Web},
 facet.year = {2009},
 id = {53.0},
 img = {researchImages/pivot.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Pivot is an experimental application for exploring large data sets with smooth visual interactions. The application originally was released by Microsoft Live Labs in October 2009, and it is being re-released by Microsoft Research to enable the research community to continue to use it for experiments. If you have Internet Explorer 9 installed, disable GPU rendering in Internet Explorer to enable Pivot to work correctly. The Pivot collection home page points to content no longer available, but Pivot still can be used for viewing user-created local or web collections. This standalone version of Pivot is unsupported and might stop functioning properly in the future.},
 pages = {},
 pdf = {},
 primary = {Visualization},
 publisher = {},
 reference = {http://research.microsoft.com/en-us/downloads/dd4a479f-92d6-496f-867d-666c87fbaada/default.aspx},
 school = {},
 text = {},
 thumb = {thumbnail/pivot.png},
 title = {LiveLabs Pivot Viewer},
 type = {software},
 video = {http://research.microsoft.com/~sdrucker/video/ThumbtackFinal/ThumbtackIntroductionVideo.wmv},
 volume = {},
 year = {2009}
}

@misc{rnviewer,
 author = {Steven M. Drucker, Curtis Wong, Steven De Mar},
 booktitle = {},
 caption = {Right Now Viewer},
 editor = {},
 facet.collaborators = {Wong,Flora},
 facet.publication = {InternalReport},
 facet.subject = {UI,TV},
 facet.year = {2002},
 id = {27.0},
 img = {researchImages/rightnow_viewer_photo.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {The purpose of this thought experiment was to look at time compression for when you turned on the TV to quickly find out what's on. Changing channels takes time and often there's a commercial on so you have to wait. This demo shows how a tuner could cache 12 most popular tv channels you watch and assemble them into a time compressed 30x real time video clip. In this prototype you can click on an individual thumbnail and it plays regular speed. The time compression inherent in the clips is long enough to transcend the commercials so you can see what's on. UI study participants were able to distinguish different TV formats (like sports vs. news), but failed to get a more detailed grasp of the program. For this the UI seemed to display too much information simultaneously. Additional experiments need to be done to find the right balance of speed, number and size of simultaneous videos playing and video thumbnail size would improve comprehension and recognition.},
 pages = {},
 pdf = {},
 primary = {Media},
 publisher = {},
 reference = {Drucker, S. Wong, C. Right Now Viewer, MS Internal Report},
 school = {},
 text = {},
 thumb = {thumbnail/rightnow_viewer_photo.jpg},
 title = {Right Now Viewer},
 type = {demo},
 video = {},
 volume = {},
 year = {2002}
}

@article{sarikayasequence,
 author = {Sarikaya, Alper and Zgraggen, Emanuel and DeLine, Rob and Drucker, Steven and Fisher, Danyel},
 booktitle = {},
 caption = {Sequence Preprocessing},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {},
 pages = {},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Sequence Pre-processing: Focusing Analysis of Log Event Data},
 type = {},
 video = {},
 volume = {},
 year = {2016}
}

@inproceedings{satpathy2008pixaura,
 author = {Satpathy, Lalatendu and Kamppari, Saara and Lewis, Bridget and Prasad, Ajay and Rhee, Yong Woo and Elgart, Benjamin and Drucker, Steven},
 booktitle = {Proceedings of the 22nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction-Volume 2},
 caption = {Pixaura},
 editor = {},
 facet.collaborators = {Elgart,Kamppari,Lewis,Prasad,Rhee,Satpathy},
 facet.publication = {HCI},
 facet.subject = {UI,Photos},
 facet.year = {2008},
 id = {49.0},
 img = {researchImages/pixAura.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {British Computer Society},
 abstract = {Current advances in digital technology promote capturing and storing more digital photos than ever. While photo collections are growing in size, the amount of time that can be devoted to viewing, managing, and sharing digital photos remains constant. Photo decision-making and selection has been identified as key to addressing this concern. After conducting exploratory research on photo decision-making including a wide-scale survey of user behaviors, detailed contextual inquiries, and longer-term diary studies, Pixaura was designed to address problems that emerged from our research. Specifically, Pixaura aims to bridge the gap between importing source photos and sharing them with others, by supporting tentative decision-making within the selection process. For this experience, the system incorporates certain core elements: 1) flexibility to experiment with relationships between photos and groups of photos, 2) the ability to closely couple photos while sharing only a subset of those photos, and 3) a tight connection between the photo selection and photo sharing space.},
 pages = {87--91},
 pdf = {http://research.microsoft.com/~sdrucker/papers/HCI_2008_TentativeDecisionsPixaurafinal.pdf},
 primary = {Photos},
 publisher = {},
 reference = {Elgart, Kamppari, Lewis, Prasad, Rhee, Satpathy, Drucker, Pixaura: Supporting Tentative Decision Making when Selecting and Sharing Digital Photos, HCI 2008},
 school = {},
 text = {HCI_2008_TentativeDecisionsPixaurafinal.txt},
 thumb = {thumbnail/pixAura.jpg},
 title = {Pixaura: supporting tentative decision making when selecting and sharing digital photos},
 type = {},
 video = {},
 volume = {},
 year = {2008}
}

@article{schroder1992data,
 author = {Schroder, Peter and Drucker, Steven},
 booktitle = {},
 caption = {Parallel Raytracing},
 editor = {},
 facet.collaborators = {Schroeder},
 facet.publication = {GI},
 facet.subject = {Graphics,Parallel Computing},
 facet.year = {1992},
 id = {7.0},
 img = {researchImages/dataparallel.gif},
 institution = {},
 journal = {Proceedings of Computer Graphics Interface},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {We describe a new data parallel algorithm for raytracing. Load balancing is achieved through the use of processor allocation, which continually remaps available resources. In this manner heterogeneous data bases are handled without the usual problems of low resource usage. The proposed approach adapts well to both extremes: a small number of rays and a large database; a large number of rays and a small database. The algorithm scales linearly|over a wide range|in the number of rays and available processors. We present an implementation on the Connection Machine CM2 system and provide timings.},
 pages = {167--175},
 pdf = {http://research.microsoft.com/~sdrucker/papers/GIraytrace.pdf},
 primary = {Graphics},
 publisher = {},
 reference = {Schroeder, P. and Drucker, S.M. Data Parallel Raytracing. Graphics Interface '92. Vancouver, B.C. 1992.},
 school = {},
 text = {GIraytrace.txt},
 thumb = {thumbnail/dataparallel.gif},
 title = {A data parallel algorithm for raytracing of heterogeneous databases},
 type = {},
 video = {},
 volume = {},
 year = {1992}
}

@misc{selfinflated,
 author = {Tinsley Galyean and Steven Drucker},
 booktitle = {},
 caption = {Self Inflated},
 editor = {},
 facet.collaborators = {Galyean},
 facet.publication = {Movie},
 facet.subject = {Graphics},
 facet.year = {1992},
 id = {6.0},
 img = {researchImages/sismall.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {A video about a republican challenger in 1992. Shown at the Democratic National Convention.},
 pages = {},
 pdf = {},
 primary = {Graphics},
 publisher = {},
 reference = {Galyean, T. & Drucker, S.M. Self Inflated Animation, 1992},
 school = {},
 text = {},
 thumb = {thumbnail/sismall.gif},
 title = {},
 type = {animation},
 video = {},
 volume = {},
 year = {1992}
}

@article{shneiderman2006find,
 author = {Shneiderman, Ben and Bederson, Benjamin B and Drucker, Steven M},
 booktitle = {},
 caption = {Find That Photo},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {Communications of the ACM},
 link = {},
 month = {},
 number = {4},
 organization = {},
 abstract = {},
 pages = {69--71},
 pdf = {},
 primary = {},
 publisher = {ACM},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Find that photo!: interface strategies to annotate, browse, and share},
 type = {},
 video = {},
 volume = {49},
 year = {2006}
}

@misc{siegel1987performance,
 author = {Siegel, David M and Drucker, Steven M and Garabieta, Inaki},
 booktitle = {},
 caption = {Tactile Sensor},
 editor = {},
 facet.collaborators = {Siegel,Garabieta},
 facet.publication = {IEEE},
 facet.subject = {Robotics,Touch},
 facet.year = {1987},
 id = {0.0},
 img = {researchImages/perfanalysis.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {This paper discusses the design of a contact sensor for use with the Utah-MIT dexterous hand [Jacobsen, et al. 1984]. The sensor utilizes an 8x8 array of capacitive cells. This paper extends the work presented in Siegel and Garabiet [1986], and the ealier work of Boie [1984]; a more detailed design analysis, modifications to the construction process, and better performance results are shown.},
 pages = {},
 pdf = {http://research.microsoft.com/copyright/accept.asp?path=/~sdrucker/papers/tactilesensor,pdf&pub=IEEE},
 primary = {Robotics},
 publisher = {IEEE},
 reference = {Siegel, D.; Drucker, S.; Garabieta, I, Performance analysis of a tactile sensor, Robotics and Automation. Proceedings. 1987 IEEE International Conference on , vol.4, no., pp.1493,1499, March 1987},
 school = {},
 text = {tactilesensor.txt},
 thumb = {thumbnail/perfanalysis.png},
 title = {Performance analysis of a tactile sensor},
 type = {},
 video = {},
 volume = {},
 year = {1987}
}

@inproceedings{smith1999counting,
 author = {Smith, Marc A and Drucker, Steven M and Kraut, Robert and Wellman, Barry},
 booktitle = {CHI'99 Extended Abstracts on Human Factors in Computing Systems},
 caption = {Counting Community},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {},
 pages = {87--88},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Counting on community in cyberspace},
 type = {},
 video = {},
 volume = {},
 year = {1999}
}

@inproceedings{smith2000social,
 author = {Smith, Marc A and Farnham, Shelly D and Drucker, Steven M},
 booktitle = {Proceedings of the SIGCHI conference on Human Factors in Computing Systems},
 caption = {Social Life of Avatars},
 editor = {},
 facet.collaborators = {Smith,Farnham},
 facet.publication = {SIGCHI},
 facet.subject = {Social},
 facet.year = {2000},
 id = {19.0},
 img = {researchImages/socialavatar.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {This paper provides a unique quantitative analysis of the social dynamics of three chat rooms in the Microsoft V-Chat graphical chat system. Survey and behavioral data were used to study user experience and activity. 150 V-Chat participants completed a web-based survey, and data logs were collected from three V-Chat rooms over the course of 119 days. This data illustrates the usage patterns of graphical chat systems, and highlights the ways physical proxemics are translated into social interactions in online Environments. V-Chat participants actively used gestures, avatars, and movement as part of their social interactions. Analyses of clustering patterns and movement data show that avatars were used to provide nonverbal cues similar to those found in face-to-face interactions. However, use of some graphical features, in particular gestures, declined as users became more experienced with the system. These findings have implications for the design and study of online interactive environments.},
 pages = {462--469},
 pdf = {http://research.microsoft.com/~sdrucker/papers/chisoclife.pdf},
 primary = {Social},
 publisher = {},
 reference = {Smith, M., Farnham, S., & Drucker S. The Social Life of Small Graphical Chat Spaces. In Proceedings of CHI 2000, The Hague, Netherlands March 2000.},
 school = {},
 text = {chisoclife.txt},
 thumb = {thumbnail/socialavatar.jpg},
 title = {The social life of small graphical chat spaces},
 type = {},
 video = {},
 volume = {},
 year = {2000}
}

@proceedings{squeries-visual-regular-expressions-for-querying-and-exploring-event-sequences,
 author = {Emanuel Zgraggen, Steven Drucker, Danyel Fisher, Rob DeLine},
 booktitle = {},
 caption = {Squeries},
 editor = {},
 facet.collaborators = {Zgraggen,Fisher,DeLine},
 facet.publication = {SIGCHI},
 facet.subject = {Information,Visualization,Touch,Sequences},
 facet.year = {2015},
 id = {79.0},
 img = {researchImages/squeries.png},
 institution = {},
 journal = {},
 link = {https://www.microsoft.com/en-us/research/publication/squeries-visual-regular-expressions-for-querying-and-exploring-event-sequences/},
 month = {April},
 number = {},
 organization = {},
 abstract = {Many different domains collect event sequence data and rely on finding and analyzing patterns within it to gain meaningful insights. Current systems that support such queries either provide limited expressiveness, hinder exploratory workflows or present interaction and visualization models which do not scale well to large and multi-faceted data sets. In this paper we present (s|qu)eries (pronounced 'Squeries'), a visual query interface for creating queries on sequences (series) of data, based on regular expressions. (s|qu)eries is a touchbased system that exposes the full expressive power of regular expressions in an approachable way and interleaves query specification with result visualizations. Being able to visually investigate the results of different query-parts supports debugging and encourages iterative query-building as well as exploratory work-flows. We validate our design and implementation through a set of informal interviews with data scientists that analyze event sequences on a daily basis.},
 pages = {},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/chi2015-squeries.pdf},
 primary = {Visualization},
 publisher = {ACM – Association for Computing Machinery},
 reference = {Zgraggen, Emanuel, Steven M. Drucker, Danyel Fisher, Rob DeLine. (s|qu)eries: Visual Regular Expressions for Querying and Exploring Event Sequences. Proceedings of ACM Conference on Human Factors in Computing Systems (CHI 2015).},
 school = {},
 text = {chi2015-squeries.txt},
 thumb = {thumbnail/squeries.png},
 title = {(s|qu)eries: Visual Regular Expressions for Querying and Exploring Event Sequences},
 type = {},
 video = {},
 volume = {},
 year = {2015}
}

@inproceedings{teevan2009visual,
 author = {Teevan, Jaime and Cutrell, Edward and Fisher, Danyel and Drucker, Steven M and Ramos, Gonzalo and Andr{\'e}, Paul and Hu, Chang},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 caption = {Visual Snippets},
 editor = {},
 facet.collaborators = {Teevan,Cutrell,Fisher,Ramos,Andre,Hu},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Information,Visualization,Design,Search},
 facet.year = {2009},
 id = {50.0},
 img = {researchImages/thumbnail_visualsnippets.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {People regularly interact with different representations of Web pages. A person looking for new information may initially find a Web page represented as a short snippet rendered by a search engine. When he wants to return to the same page the next day, the page may instead be represented by a link in his browser history. Previous research has explored how to best represent Web pages in support of specific task types, but, as we find in this paper, consistency in representation across tasks is also important. We explore how different representations are used in a variety of contexts and present a compact representation that supports both the identification of new, relevant Web pages and the re-finding of previously viewed pages.},
 pages = {2023--2032},
 pdf = {http://research.microsoft.com/~sdrucker/papers/2008CHI_VisualSnippets.pdf},
 primary = {Visualization},
 publisher = {},
 reference = {Teevan, J. Cutrell, E., Fisher, D., Drucker, S.M., Ramos, G., Andre, P., Hu, C., Visual Snippets: Summarizing Web Pages for Search and Revisitation},
 school = {},
 text = {2008CHI_VisualSnippets.txt},
 thumb = {thumbnail/thumbnail_visualsnippets.png},
 title = {Visual snippets: summarizing web pages for search and revisitation},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/facetPatchCameraReady.mov},
 volume = {},
 year = {2009}
}

@incollection{texturefromtouch,
 author = {},
 booktitle = {Natural Computation},
 caption = {Texture from Touch},
 editor = {Whitman Richards},
 facet.collaborators = {Individual},
 facet.publication = {Book},
 facet.subject = {Robotics,Touch},
 facet.year = {1988},
 id = {1.0},
 img = {researchImages/natcomp.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Not available},
 pages = {},
 pdf = {},
 primary = {Robotics},
 publisher = {MIT Press},
 reference = {Steven M. Drucker. Texture from Touch. In: Whitman Richards, editor. Natural Computation. MIT Press; 1988. },
 school = {},
 text = {},
 thumb = {thumbnail/natcomp.png},
 title = {Texture from Touch},
 type = {},
 video = {},
 volume = {},
 year = {1988}
}

@misc{title={SandDance,
 author = {Steven M. Drucker and Roland Fernandez},
 booktitle = {},
 caption = {SandDance},
 editor = {},
 facet.collaborators = {Fernandez,Fisher},
 facet.publication = {InternalReport},
 facet.subject = {UI,Information,Visualization,Touch},
 facet.year = {2013},
 id = {72.0},
 img = {researchImages/sanddance.png},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {SandDance is a browser based information visualization system prototype created at Microsoft Research that scales to hundreds of thousands of items. Arbitrary datatables can be loaded and results can be filtered using facets and search and displayed in a variety of layouts. Transitions between the views are animated so that users can better maintain context. Multiple linked views allow for associations between the same items in each view. Multiple devices can simultaneusly interact with each other on the same dataset.  Using principles of information visualization, users can map any attribute into the position, color, size, opacity and layout of a dataset to help reveal patterns within the data. SandDance lets you see both the individual records, and their overall structure.  SandDance focusses on natural user interaction techniques. Touch interaction is a first class citizen, allowing the entire experience to be easily operated through a touch screen. The system also understand speech commands for searching, selecting, focusing and filtering the data. A kinect system can be used to sense gestures for moving between views of the data. Collaboration is supported by allowing multiple sets of people to interact with the same dataset. Selections and filters in one system are automatically replicated to other systems viewing the data. },
 pages = {},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/SandDance/},
 primary = {Visualization},
 publisher = {},
 reference = {Unpublished},
 school = {},
 text = {sanddance.txt},
 thumb = {thumbnail/sanddance.png},
 title = {},
 type = {software},
 video = {http://research.microsoft.com/~sdrucker/video/TouchViz2.mp4},
 volume = {},
 year = {2013}
}

@techreport{tokentv,
 author = {Steven M. Drucker, Curtis Wong},
 booktitle = {},
 caption = {Token TV},
 editor = {},
 facet.collaborators = {Wong},
 facet.publication = {InternalReport},
 facet.subject = {UI,TV,Media},
 facet.year = {2001},
 id = {21.0},
 img = {researchImages/tokenTV_photo.jpg},
 institution = {Microsoft Research},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {TV Tokens (GUID for a specific broadcast program or movie) can be embedded in any website, EPG or email, downloaded and shared between friends to send to respective PVR's to schedule recording of show. TokenTV service (dot.NET TV) converts GUID to resolve to local schedule information needed to program the PVR. Any content based website (i.e.: IBDB.com, PBS.org, AFI.org) could have tokens to download to PVR for recording specific content.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/tokentv.doc},
 primary = {Media},
 publisher = {},
 reference = {Drucker, S.M., Wong, C. 2001, Token TV: Sharing preferences for Television DVR Recording, MS Technical Report},
 school = {},
 text = {},
 thumb = {thumbnail/tokenTV_photo.jpg},
 title = {Token TV: Sharing preferences for Television DVR recording},
 type = {},
 video = {},
 volume = {},
 year = {2001}
}

@inproceedings{toomim2009attaching,
 author = {Toomim, Michael and Drucker, Steven M and Dontcheva, Mira and Rahimi, Ali and Thomson, Blake and Landay, James A},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 caption = {reForm},
 editor = {},
 facet.collaborators = {Toomim,Dontcheva,Rahimi,Thomson,Landay},
 facet.publication = {SIGCHI},
 facet.subject = {UI,Information,Web},
 facet.year = {2009},
 id = {52.0},
 img = {researchImages/thumbnail_reform.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {There are not enough programmers to support all end user goals by building websites, mashups, and browser extensions. This paper presents reform, a system that envisions roles for both programmers and end-users in creating enhancements of existing websites that support new goals. Programmers author a traditional mashup or browser extension, but instead of writing a web scraper by hand, the reform system enables novice end users to attach the mashup to their websites of interest. reform both makes scraping easier for the programmer and carries the benefit that endusers can retarget the enhancements towards completely different web sites, using a new programming by example interface and machine learning algorithm for web data extraction. This work presents reform's architecture, algorithms, user interface, evaluation, and five example reform enabled enhancements that provide a step towards our goal of write-once apply-anywhere user interface enhancements.},
 pages = {1859--1868},
 pdf = {http://research.microsoft.com/~sdrucker/papers/2008CHI_reform.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Toomim, M., Drucker, S.M., Dontcheva, M., Rahimi, A., Thomson, B., Landay, J.A., Attaching UI Enhancements to Websites with End Users, SIGCHI 2009.},
 school = {},
 text = {2008CHI_reform.txt},
 thumb = {thumbnail/thumbnail_reform.jpg},
 title = {Attaching UI enhancements to websites with end users},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/video/ThumbtackFinal/ThumbtackIntroductionVideo.wmv},
 volume = {},
 year = {2009}
}

@techreport{understanding-breadth-event-space-learning-logan,
 author = {Danyel Fisher, Steven Drucker, Mary Czerwinski, Rob DeLine, Kael Rowan},
 booktitle = {Proceedings of the Event Event: Temporal & Sequential Event Analysis. IEEE VIS 2016 Workshop},
 caption = {Logan},
 editor = {},
 facet.collaborators = {Fisher,DeLine,Czerwinski},
 facet.publication = {Infovis},
 facet.subject = {Information,Visualization,Sequences},
 facet.year = {2016},
 id = {82.0},
 img = {researchImages/logan.png},
 institution = {},
 journal = {},
 link = {https://www.microsoft.com/en-us/research/publication/understanding-breadth-event-space-learning-logan/},
 month = {October},
 number = {},
 organization = {},
 abstract = {Event processing, analysis, and visualization are increasingly important, and common, problems as telemetry and log recording become ubiquitous. We are still learning about the space of ways to both query and see the results of sequential queries against event-logged data. In this paper, we discuss our Logan event exploration prototype, which is based on regular-expression queries and result histograms; we then discuss the many factors that vary between tools, domains, and datasets ? tradeoffs of factors such as session size, cardinality of events, and the presence of event arguments.},
 pages = {},
 pdf = {https://www.microsoft.com/en-us/research/publication/understanding-breadth-event-space-learning-logan/},
 primary = {Visualization},
 publisher = {},
 reference = {Fisher, Drucker, DeLine, Czerwinsiki, Understanding the Breadth of the Event Space, Learnings from Logan},
 school = {},
 text = {EVENT_2016_paper_17.txt},
 thumb = {thumbnail/logan.png},
 title = {Understanding the Breadth of the Event Space: Learning from Logan},
 type = {},
 video = {},
 volume = {},
 year = {2016}
}

@inproceedings{vellon1998architecture,
 author = {Vellon, Manny and Marple, Kirk and Mitchell, Don and Drucker, Steven},
 booktitle = {COOTS},
 caption = {VWorlds},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {}, 
 abstract = {},
 pages = {211--218},
 pdf = {},
 primary = {},
 publisher = {},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {The Architecture of a Distributed Virtual Worlds System.},
 type = {},
 video = {},
 volume = {},
 year = {1998}
}

@incollection{virtualfootball,
 author = {},
 booktitle = {Intelligent camera control for graphical environments},
 caption = {Virtual Football},
 editor = {},
 facet.collaborators = {Individual},
 facet.publication = {Thesis},
 facet.subject = {Graphics,Camera,Thesis},
 facet.year = {1994},
 id = {12.0},
 img = {researchImages/footballsm.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {Too often in the field of computer graphics, practitioners have been more concerned with the question of how to move a camera rather than why to move it. This thesis addresses the core question of why the camera is being placed and moved and uses answers to that question to provide a more convenient, more intelligent method for controlling virtual cameras in computer graphics. After discussing the general sorts of activities to be performed in graphical environments, this thesis then contains a derivation of some camera primitives that are required, and examines how they can be incorporated into different interfaces. A single, consistent, underlying framework for camera control across many different domains has been posited and formulated in terms of constrained optimization. Examples from different application domains demonstrate a variety of interface styles that have all been implemented on top of the underlying framework. Evaluations for each application are also given.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/thesiswbmakrs.pdf},
 primary = {Camera},
 publisher = {MIT},
 reference = {Drucker, S.M. Intelligent Camera Control for Graphical Environments PhD Thesis, MIT Media Lab. 1994.},
 school = {},
 text = {thesiswbmakrs.txt},
 thumb = {thumbnail/footballsm.gif},
 title = {Virtual Football}
 author={Drucker, Steven Mark},
 type = {},
 video = {},
 volume = {},
 year = {1994}
}

@incollection{virtualfootball,
 author = {},
 booktitle = {Intelligent camera control for graphical environments},
 caption = {CamDroid},
 editor = {},
 facet.collaborators = {Zeltzer},
 facet.publication = {SIGGRAPH,Thesis},
 facet.subject = {Graphics,Camera,Thesis},
 facet.year = {1995},
 id = {13.0},
 img = {researchImages/camdroid.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {In this paper, a method of encapsulating camera tasks into well defined units called camera modules is described. Through this encapsulation, camera modules can be programmed and sequenced, and thus can be used as the underlying framework for controlling the virtual camera in widely disparate types of graphical environments. Two examples of the camera framework are shown: an agent which can film a conversation between two virtual actors and a visual programming language for filming a virtual football game.},
 pages = {},
 pdf = {http://research.microsoft.com/~sdrucker/papers/SIG95symp.pdf},
 primary = {Camera},
 publisher = {MIT},
 reference = {Drucker, S.M. and Zeltzer, D. CamDroid: A System for InEelligent Camera Control. SIGGRAPH Symposium on Interactive 3D Graphics, 1995.},
 school = {},
 text = {SIG95symp.txt},
 thumb = {thumbnail/camdroid.gif},
 title = {CamDroid},
 author={Drucker, Steven Mark},
 type = {},
 video = {},
 volume = {},
 year = {1994}
}

@inproceedings{vronay1999alternative,
 author = {Vronay, David and Smith, Marc and Drucker, Steven},
 booktitle = {Proceedings of the 12th annual ACM symposium on User interface software and technology},
 caption = {Streaming Chat},
 editor = {},
 facet.collaborators = {Vronay,Smith},
 facet.publication = {UIST},
 facet.subject = {UI,Social},
 facet.year = {1999},
 id = {14.0},
 img = {researchImages/streamChat.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {We describe some common problems experienced by users of computer-based text chat, and show how many of these problems relate to the loss of timing-specific information.  We suggest that thinking of chat as a streaming media data type might solve some of these problems.  We then present a number of alternative chat interfaces along with results from user studies comparing and contrasting them both with each other and with the standard chat interface.},
 pages = {19--26},
 pdf = {http://research.microsoft.com/~sdrucker/papers/chat.pdf},
 primary = {Social},
 publisher = {},
 reference = {David Vronay, Marc Smith, and Steven M. Drucker, Chat as a Streaming Media Data Type, UIST. 1999.},
 school = {},
 text = {chat.txt},
 thumb = {thumbnail/streamChat.jpg},
 title = {Alternative interfaces for chat},
 type = {},
 video = {},
 volume = {},
 year = {1999}
}

@inproceedings{wang2006cartoon,
 author = {Wang, Jue and Drucker, Steven M and Agrawala, Maneesh and Cohen, Michael F},
 booktitle = {ACM Transactions on Graphics (TOG)},
 caption = {Cartoon Animation Filter},
 editor = {},
 facet.collaborators = {Wang,Agrawala,Cohen},
 facet.publication = {SIGGRAPH},
 facet.subject = {Graphics,Animation},
 facet.year = {2006},
 id = {42.0},
 img = {researchImages/aniequation.jpg},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {3},
 organization = {ACM},
 abstract = {We present the 'Cartoon Animation Filter', a simple filter that takes an arbitrary input motion signal and modulates it in such a way that the output motion is more 'alive' or 'animated'. The filter adds a smoothed, inverted, and (sometimes) time shifted version of the second derivative (the acceleration) of the signal back into the original signal. Almost all parameters of the filter are automated. The user only needs to set the desired strength of the filter. The beauty of the animation filter lies in its simplicity and generality. We apply the filter to motions ranging from hand drawn trajectories, to simple animations within PowerPoint presentations, to motion captured DOF curves, to video segmentation results. Experimental results show that the filtered motion exhibits anticipation, follow-through, exaggeration and squash-and-stretch effects which are not present in the original input motion data.},
 pages = {1169--1173},
 pdf = {http://research.microsoft.com/copyright/accept.asp?path=/~sdrucker/papers/TheCartoonAnimationFilter.pdf&pub=ACM},
 primary = {Graphics},
 publisher = {},
 reference = {Jue Wang, Steven Drucker, Maneesh Agrawala, Michael Cohen, The Cartoon Animation Filter, ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006), July 2006},
 school = {},
 text = {TheCartoonAnimationFilter.txt},
 thumb = {thumbnail/aniequation.jpg},
 title = {The cartoon animation filter},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/Video/af.mov},
 volume = {25},
 year = {2006}
}

@article{white2006supporting,
 author = {White, Ryen W and Kules, Bill and Drucker, Steven M and others},
 booktitle = {},
 caption = {Exploratory Search},
 editor = {},
 facet.collaborators = {White,Kules,schraefel},
 facet.publication = {ACM},
 facet.subject = {UI,Search},
 facet.year = {2006},
 id = {38.0},
 img = {researchImages/CACM.png},
 institution = {},
 journal = {Communications of the ACM},
 link = {},
 month = {},
 number = {4},
 organization = {},
 abstract = {},
 pages = {36--39},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/2007introacm.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {White RW, Kules B, Drucker SM, schraefel M. Introduction. Commun. ACM. 2006;49(4):36-39.},
 school = {},
 text = {2007introacm.txt},
 thumb = {thumbnail/CACM.png},
 title = {Supporting exploratory search, introduction, special issue, communications of the ACM},
 type = {},
 video = {},
 volume = {49},
 year = {2006}
}

@inproceedings{white2007exploratory,
 author = {White, Ryen W and Drucker, Steven M and Marchionini, Gary and Hearst, Marti and others},
 booktitle = {CHI'07 Extended Abstracts on Human Factors in Computing Systems},
 caption = {Search Workshop},
 editor = {},
 facet.collaborators = {White,Marchionini,Hearst,schraefel},
 facet.publication = {SIGCHI},
 facet.subject = {Visualization,UI,Search,Web},
 facet.year = {2007},
 id = {47.0},
 img = {researchImages/ExpSearchint.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {The model of search as a turn-taking dialogue between the user and an intermediary has remained unchanged for decades. However, there is growing interest within the search community in evolving this model to support search-driven information exploration activities. So-called exploratory search describes a class of search activities that move beyond fact retrieval toward fostering learning, investigation, and information use. Exploratory search interaction focuses on the user-system communication essential during exploratory search processes. Given this user-centered focus, the CHI conference is an ideal venue to discuss mechanisms to support exploratory searchbehaviors. Specifically, this workshop aims to gather researchers, academics, and practitioners working in human-computer interaction, information retrieval, and other related disciplines, for a discussion of the issues relating to the design and evaluation of interfaces to help users explore, learn, and use information. These are important issues with far-reaching implications for how many computer users accomplish their tasks.},
 pages = {2877--2880},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/p2877-white.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {White RW, Drucker SM, Marchionini G, Hearst M, schraefel MC. Exploratory search and HCI: designing and evaluating interfaces to support exploratory search interaction [Internet]. In: CHI '07 extended abstracts on Human factors in computing systems. San Jose, CA, USA: ACM; 2007 [cited 2010 Aug 10]. p. 2877-2880},
 school = {},
 text = {p2877-white.txt},
 thumb = {thumbnail/ExpSearchint.png},
 title = {Exploratory search and HCI: designing and evaluating interfaces to support exploratory search interaction},
 type = {},
 video = {http://research.microsoft.com/~sdrucker/papers/uist2008annotating.pdf},
 volume = {},
 year = {2007}
}

@inproceedings{white2007investigating,
 author = {White, Ryen W and Drucker, Steven M},
 booktitle = {Proceedings of the 16th international conference on World Wide Web},
 caption = {Web Search Variability},
 editor = {},
 facet.collaborators = {White},
 facet.publication = {WWW},
 facet.subject = {Visualization,UI,Search,Web},
 facet.year = {2007},
 id = {45.0},
 img = {researchImages/websearch.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people's interaction behavior when engaged in search-related activities on the Web. We analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles. The findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit. Our findings also suggest two classes of extreme user navigators and explorers whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone.},
 pages = {21--30},
 pdf = {http://research.microsoft.com/en-us/um/people/ryenw/talks/pdf/WhiteDruckerWWW2007.pdf},
 primary = {UI-Information},
 publisher = {},
 reference = {Investigating Behavioral Variability in Web Search, Ryen W. White and Steven M. Drucker  16th International World Wide Web Conference (WWW 2007), Pages: 21-30 Banff, Canada, May 2007},
 school = {},
 text = {WhiteWWW2007.txt},
 thumb = {thumbnail/websearch.png},
 title = {Investigating behavioral variability in web search},
 type = {},
 video = {},
 volume = {},
 year = {2007}
}

@incollection{whitted2012visualization,
 author = {Whitted, Turner and Drucker, Steven},
 booktitle = {},
 caption = {},
 editor = {},
 facet.collaborators = {},
 facet.publication = {},
 facet.subject = {},
 facet.year = {},
 id = {},
 img = {},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract={Large displays suitable for visualization applications are typically constructed from arrays of smaller ones. The physical and optical challenges of designing these assemblies are the first topic addressed here.},
 booktitle={Expanding the Frontiers of Visual Analytics and Visualization},
 pages = {417--427},
 pdf = {},
 primary = {},
 publisher = {Springer London},
 reference = {},
 school = {},
 text = {},
 thumb = {},
 title = {Visualization Surfaces},
 type = {},
 video = {},
 volume = {},
 year = {2012}
}

@article{xiong1998visualizations,
 author = {Xiong, Rebecca and Smith, Marc A and Drucker, Steven M},
 booktitle = {},
 caption = {Collaborative Visualization},
 editor = {},
 facet.collaborators = {Xiong,Smith},
 facet.publication = {InternalReport},
 facet.subject = {Visualization,Social},
 facet.year = {1999},
 id = {15.0},
 img = {researchImages/collab.jpg},
 institution = {},
 journal = {Microsoft Research},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {There is a growing need for methods and tools to illuminate the social contexts of interaction environments created by the World Wide Web, Usenet newsgroups, email lists, and other network interaction media. We present here a framework for creating visualizations of the social connections created in and through network interaction media. Using graph-drawing methods, visualizations can be created for a range of systems that link people to people and people to objects through networks. As an example, we present an application of our methods to the Usenet to illustrate how visualization can improve existing systems.  We propose that users of network interaction media can benefit from visualizations that illuminate the interaction context generated by the rich interconnections between groups, conversations, and people in these media.},
 pages = {1--8},
 pdf = {http://research.microsoft.com/~sdrucker/papers/collabviz.pdf},
 primary = {Social},
 publisher = {},
 reference = {Rebecca Xiong, Marc A. Smith,Steven M. Drucker, Visualizations of Collaborative Information for End-Users,  Internal Report, 1999.},
 school = {},
 text = {collabviz.txt},
 thumb = {thumbnail/collab.jpg},
 title = {Visualizations of collaborative information for end-users},
 type = {},
 video = {},
 volume = {},
 year = {1998}
}

@article{yankelovich1987connections,
 author = {Yankelovich, Nicole and Haan, Bernard and Drucker, Stephen},
 booktitle = {},
 caption = {Intermedia2},
 editor = {},
 facet.collaborators = {Yankelovich,Haan},
 facet.publication = {ICSS},
 facet.subject = {Hyptertext,UI,Information},
 facet.year = {1988},
 id = {3.0},
 img = {researchImages/intermedia2.gif},
 institution = {},
 journal = {Providence, Rhode Island: Institute for Research in Information and Scholarship, Brown University},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {none},
 pages = {},
 pdf = {http://research.microsoft.com/copyright/accept.asp?path=/~sdrucker/papers/connintermedia.pdf&pub=IEEE},
 primary = {Hypertext},
 publisher = {},
 reference = {Yankelovich, N, Haan, B.J., and Drucker, S.M., Connections in Context: the Intermedia Systems. International Conference on Systems Sciences, Vol. 2 pp. 715-724, January: 1988.},
 school = {},
 text = {intermedia1.txt},
 thumb = {thumbnail/intermedia2.gif},
 title = {Connections in Context: The intermedia system},
 type = {},
 video = {},
 volume = {},
 year = {1987}
}

@article{yankelovich1988intermedia,
 author = {Yankelovich, Nicole and Haan, Bernard J. and Meyrowitz, Norman K. and Drucker, Steven M.},
 booktitle = {},
 caption = {Intermedia1},
 editor = {},
 facet.collaborators = {Yankelovich,Haan,Meyrowitz},
 facet.publication = {IEEE},
 facet.subject = {Hyptertext,UI,Information},
 facet.year = {1988},
 id = {2.0},
 img = {researchImages/intermedia.gif},
 institution = {},
 journal = {IEEE computer},
 link = {},
 month = {},
 number = {1},
 organization = {},
 abstract = {none},
 pages = {81--96},
 pdf = {http://research.microsoft.com/copyright/accept.asp?path=/~sdrucker/papers/intermedia1.pdf&pub=IEEE},
 primary = {Hypertext},
 publisher = {},
 reference = {Yankelovich, N, Haan, B.J., Meyrowitz, N. and Drucker, S.M., INTERMEDIA: The Concept and Construction of a Seamless Environment. IEEE Computer, January: 1988.},
 school = {},
 text = {connintermedia.txt},
 thumb = {thumbnail/intermedia.gif},
 title = {Intermedia: The concept and the construction of a seamless information environment},
 type = {},
 video = {},
 volume = {21},
 year = {1988}
}

@inproceedings{zeltzer1992virtual,
 author = {Zeltzer, David and Drucker, Steven},
 booktitle = {Proceedings of the IMAGE VI conference},
 caption = {Mission Planner},
 editor = {},
 facet.collaborators = {Zeltzer},
 facet.publication = {Thesis},
 facet.subject = {Graphics,Camera,Thesis},
 facet.year = {1994},
 id = {10.0},
 img = {researchImages/darpa.gif},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {},
 abstract = {A key function of a mission planning system is to enhance and maintain situational awareness of planning personnel and aircrews who will use the system for pre-mission rehearsals and briefings. We have developed a mission planner using virtual environment technology. We provide a task level interface to computational models of aircraft, terrain, threats and targets, so that users interact directly with these models using voice and gesture recognition, 3D positional input, 3 axis force output, and intelligent camera control.},
 pages = {125--134},
 pdf = {http://research.microsoft.com/~sdrucker/papers/mission.pdf},
 primary = {Camera},
 publisher = {},
 reference = {Zeltzer, D. and Drucker, S.M. A Virtual Environment System for Mission Planning. Proc. 1992 IMAGE VI Conference. Phoenix, AZ. 1992. },
 school = {},
 text = {mission.txt},
 thumb = {thumbnail/darpa.gif},
 title = {A virtual environment system for mission planning},
 type = {},
 video = {},
 volume = {},
 year = {1992}
}

@article{zgraggen2014panoramicdata,
 author = {Zgraggen, Emanuel and Zeleznik, Robert and Drucker, Steven M},
 booktitle = {},
 caption = {Panoramic Data},
 editor = {},
 facet.collaborators = {Zgraggen,Zeleznik},
 facet.publication = {Infovis},
 facet.subject = {Information,Visualization,Touch},
 facet.year = {2014},
 id = {74.0},
 img = {researchImages/panodata.png},
 institution = {},
 journal = {IEEE transactions on visualization and computer graphics},
 link = {},
 month = {},
 number = {12},
 organization = {},
 abstract = {Interactively exploring multidimensional datasets requires frequent switching among a range of distinct but inter-related tasks (e.g., producing different visuals based on different column sets, calculating new variables, and observing the interactions between sets of data). Existing approaches either target specific different problem domains (e.g., data-transformation or datapresentation) or expose only limited aspects of the general exploratory process; in either case, users are forced to adopt coping strategies (e.g., arranging windows or using undo as a mechanism for comparison instead of using side-by-side displays) to compensate for the lack of an integrated suite of exploratory tools. PanoramicData (PD) addresses these problems by unifying a comprehensive set of tools for visual data exploration into a hybrid pen and touch system designed to exploit the visualization advantages of large interactive displays. PD goes beyond just familiar visualizations by including direct UI support for data transformation and aggregation, filtering and brushing. Leveraging an unbounded whiteboard metaphor, users can combine these tools like building blocks to create detailed interactive visual display networks in which each visualization can act as a filter for others. Further, by operating directly on relational-databases, PD provides an approachable visual language that exposes a broad set of the expressive power of SQL, including functionally complete logic filtering, computation of aggregates and natural table joins. To understand the implications of this novel approach, we conducted a formative user study with both data and visualization experts. The results indicated that the system provided a fluid and natural user experience for probing multi-dimensional data and was able to cover the full range of queries that the users wanted to pose},
 pages = {2112--2121},
 pdf = {http://research.microsoft.com/en-us/um/people/sdrucker/papers/PanoramicData.pdf},
 primary = {Visualization},
 publisher = {IEEE},
 reference = {Zgraggen, Emanuel, Robert Zeleznik, and Steven M. Drucker. PanoramicData: Data Analysis through Pen & Touch. (2014).},
 school = {},
 text = {PanoramicData.txt},
 thumb = {thumbnail/panodata.png},
 title = {PanoramicData: Data analysis through pen \& touch},
 type = {},
 video = {http://research.microsoft.com/en-us/um/people/shliu/TextPioneerTKDE.pdf},
 volume = {20},
 year = {2014}
}

@inproceedings{zhao2012timeslice,
 author = {Zhao, Jian and Drucker, Steven M and Fisher, Danyel and Brinkman, Donald},
 booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
 caption = {TimeSlice},
 editor = {},
 facet.collaborators = {Zhao,Fisher,Brinkman},
 facet.publication = {AVI},
 facet.subject = {UI,Information,Visualization,Temporal},
 facet.year = {2012},
 id = {65.0},
 img = {researchImages/timeslice.png},
 institution = {},
 journal = {},
 link = {},
 month = {},
 number = {},
 organization = {ACM},
 abstract = {Temporal events with multiple sets of metadata attributes, i.e., facets, are ubiquitous across different domains. The capabilities of efficiently viewing and comparing events data from various perspectives are critical for revealing relationships, making hypotheses, and discovering patterns. In this paper, we present TimeSlice, an interactive faceted visualization of temporal events, which allows users to easily compare and explore timelines with different attributes on a set of facets. By directly manipulating the filtering tree, a dynamic visual representation of queries and filters in the facet space, users can simultaneously browse the focused timelines and their contexts at different levels of detail, which supports efficient navigation of multi-dimensional events data. Also presented is an initial evaluation of TimeSlice with two datasets - famous deceased people and US daily flight delays.},
 pages = {433--436},
 pdf = {http://research.microsoft.com/~sdrucker/papers/timeslice.pdf},
 primary = {Visualization},
 publisher = {},
 reference = {Jian Zhao, Steven Drucker, Danyel Fisher, Donald Brinkman. TimeSlice: Interactive Faceted Browsing of Timeline Data. In AVI'12: Proceedings of the International Working Conference on Advanced Visual Interfaces, pp. 433-436, May 2012.},
 school = {},
 text = {timeslice.txt},
 thumb = {thumbnail/timeslice.png},
 title = {TimeSlice: Interactive faceted browsing of timeline data},
 type = {},
 video = {},
 volume = {},
 year = {2012}
}

